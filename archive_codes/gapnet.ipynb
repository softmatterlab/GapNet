{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot==1.2.3 in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot==1.2.3) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fontconfig fontconfig-config fonts-dejavu-core fonts-liberation libann0\n",
      "  libbsd0 libcairo2 libcdt5 libcgraph6 libdatrie1 libfontconfig1 libfreetype6\n",
      "  libgd3 libgraphite2-3 libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libharfbuzz0b\n",
      "  libice6 libjbig0 libjpeg-turbo8 libjpeg8 liblab-gamut1 libltdl7\n",
      "  libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
      "  libpixman-1-0 libpng16-16 libsm6 libthai-data libthai0 libtiff5 libwebp6\n",
      "  libx11-6 libx11-data libxau6 libxaw7 libxcb-render0 libxcb-shm0 libxcb1\n",
      "  libxdmcp6 libxext6 libxmu6 libxpm4 libxrender1 libxt6 multiarch-support ucf\n",
      "  x11-common\n",
      "Suggested packages:\n",
      "  gsfonts graphviz-doc libgd-tools\n",
      "The following NEW packages will be installed:\n",
      "  fontconfig fontconfig-config fonts-dejavu-core fonts-liberation graphviz\n",
      "  libann0 libbsd0 libcairo2 libcdt5 libcgraph6 libdatrie1 libfontconfig1\n",
      "  libfreetype6 libgd3 libgraphite2-3 libgts-0.7-5 libgts-bin libgvc6 libgvpr2\n",
      "  libharfbuzz0b libice6 libjbig0 libjpeg-turbo8 libjpeg8 liblab-gamut1\n",
      "  libltdl7 libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
      "  libpixman-1-0 libpng16-16 libsm6 libthai-data libthai0 libtiff5 libwebp6\n",
      "  libx11-6 libx11-data libxau6 libxaw7 libxcb-render0 libxcb-shm0 libxcb1\n",
      "  libxdmcp6 libxext6 libxmu6 libxpm4 libxrender1 libxt6 multiarch-support ucf\n",
      "  x11-common\n",
      "0 upgraded, 53 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 837 kB/8119 kB of archives.\n",
      "After this operation, 29.4 MB of additional disk space will be used.\n",
      "Ign:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "Ign:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "Ign:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtiff5 amd64 4.0.9-5ubuntu0.3\n",
      "Err:1 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-data all 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:2 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libx11-6 amd64 2:1.6.4-3ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "Err:3 http://security.ubuntu.com/ubuntu bionic-updates/main amd64 libtiff5 amd64 4.0.9-5ubuntu0.3\n",
      "  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-data_1.6.4-3ubuntu0.3_all.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/libx/libx11/libx11-6_1.6.4-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/t/tiff/libtiff5_4.0.9-5ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.88.142 80]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (8.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->seaborn) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot==1.2.3\n",
    "!apt-get install -y graphviz\n",
    "!pip install pandas\n",
    "!pip install -U scikit-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import load\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simulated dataset\n",
    "#feature_num = 40\n",
    "#n_samples = 1000\n",
    "#informative_num = 25\n",
    "#n_clusters_per_class = 3\n",
    "#n_redundant = 10\n",
    "#feature_num_modality_1 = 25\n",
    "#feature_num_modality_2 = 15\n",
    "#n_classes = 2\n",
    "#X, y = make_classification(n_samples=n_samples, n_classes=n_classes ,n_features=feature_num, n_redundant=n_redundant, n_informative=informative_num,\n",
    "#                            n_clusters_per_class=n_clusters_per_class)\n",
    "# fill in missing values\n",
    "#mismatch = 450\n",
    "#X[0:mismatch, 0:feature_num_modality_1] = np.nan\n",
    "#X[-mismatch:, -feature_num_modality_2:] = np.nan\n",
    "#X_overlap = X[mismatch:-mismatch]\n",
    "#y_overlap = y[mismatch:-mismatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and fill in missing values\n",
    "from numpy import load\n",
    "mismatch = 450\n",
    "X = load('archived_data_and_results/simulation_result/X.npy')\n",
    "y = load('archived_data_and_results/simulation_result/y.npy')\n",
    "X_overlap = X[mismatch:-mismatch]\n",
    "y_overlap = y[mismatch:-mismatch]\n",
    "feature_num_modality_1 = 25\n",
    "feature_num_modality_2 = 15\n",
    "feature_num = 40\n",
    "n_classes = 2\n",
    "#X_overlap = load('simulation_result/X_overlap.npy')\n",
    "#y_overlap = load('simulation_result/y_overlap.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "1  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94]\n",
      "train_accuracy 0.988+/-0.000 : [0.988]\n",
      "val_accuracy 0.900+/-0.000 : [0.9]\n",
      "val_auc 0.890+/-0.000 : [0.89]\n",
      "val_sens 0.900+/-0.000 : [0.9]\n",
      "val_spec 0.900+/-0.000 : [0.9]\n",
      "val_prec 0.900+/-0.000 : [0.9]\n",
      "Vanilla Results :\n",
      "best_epochs [6]\n",
      "train_accuracy 0.650+/-0.000 : [0.65]\n",
      "val_accuracy 0.500+/-0.000 : [0.5]\n",
      "val_auc 0.610+/-0.000 : [0.61]\n",
      "val_sens 0.500+/-0.000 : [0.5]\n",
      "val_spec 0.500+/-0.000 : [0.5]\n",
      "val_prec 0.400+/-0.000 : [0.4]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "2  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161]\n",
      "train_accuracy 0.994+/-0.006 : [0.988 1.   ]\n",
      "val_accuracy 0.925+/-0.025 : [0.9  0.95]\n",
      "val_auc 0.945+/-0.055 : [0.89 1.  ]\n",
      "val_sens 0.905+/-0.005 : [0.9   0.909]\n",
      "val_spec 0.950+/-0.050 : [0.9 1. ]\n",
      "val_prec 0.950+/-0.050 : [0.9 1. ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27]\n",
      "train_accuracy 0.819+/-0.169 : [0.65  0.988]\n",
      "val_accuracy 0.575+/-0.075 : [0.5  0.65]\n",
      "val_auc 0.675+/-0.065 : [0.61 0.74]\n",
      "val_sens 0.583+/-0.083 : [0.5   0.667]\n",
      "val_spec 0.568+/-0.068 : [0.5   0.636]\n",
      "val_prec 0.500+/-0.100 : [0.4 0.6]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30511598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e503df510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8]\n",
      "train_accuracy 0.871+/-0.174 : [0.988 1.    0.625]\n",
      "val_accuracy 0.767+/-0.225 : [0.9  0.95 0.45]\n",
      "val_auc 0.782+/-0.236 : [0.89  1.    0.455]\n",
      "val_sens 0.742+/-0.230 : [0.9   0.909 0.417]\n",
      "val_spec 0.800+/-0.216 : [0.9 1.  0.5]\n",
      "val_prec 0.819+/-0.190 : [0.9   1.    0.556]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2]\n",
      "train_accuracy 0.717+/-0.200 : [0.65  0.988 0.512]\n",
      "val_accuracy 0.533+/-0.085 : [0.5  0.65 0.45]\n",
      "val_auc 0.564+/-0.165 : [0.61  0.74  0.343]\n",
      "val_sens 0.528+/-0.104 : [0.5   0.667 0.417]\n",
      "val_spec 0.545+/-0.064 : [0.5   0.636 0.5  ]\n",
      "val_prec 0.519+/-0.086 : [0.4   0.6   0.556]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3070d488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac50a510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "4  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65]\n",
      "train_accuracy 0.900+/-0.159 : [0.988 1.    0.625 0.988]\n",
      "val_accuracy 0.788+/-0.198 : [0.9  0.95 0.45 0.85]\n",
      "val_auc 0.816+/-0.213 : [0.89  1.    0.455 0.919]\n",
      "val_sens 0.756+/-0.201 : [0.9   0.909 0.417 0.8  ]\n",
      "val_spec 0.825+/-0.192 : [0.9 1.  0.5 0.9]\n",
      "val_prec 0.836+/-0.168 : [0.9   1.    0.556 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4]\n",
      "train_accuracy 0.712+/-0.173 : [0.65  0.988 0.512 0.7  ]\n",
      "val_accuracy 0.487+/-0.108 : [0.5  0.65 0.45 0.35]\n",
      "val_auc 0.494+/-0.188 : [0.61  0.74  0.343 0.283]\n",
      "val_sens 0.396+/-0.246 : [0.5   0.667 0.417 0.   ]\n",
      "val_spec 0.518+/-0.073 : [0.5   0.636 0.5   0.438]\n",
      "val_prec 0.389+/-0.236 : [0.4   0.6   0.556 0.   ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3070d7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50112510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303]\n",
      "train_accuracy 0.920+/-0.148 : [0.988 1.    0.625 0.988 1.   ]\n",
      "val_accuracy 0.800+/-0.179 : [0.9  0.95 0.45 0.85 0.85]\n",
      "val_auc 0.849+/-0.201 : [0.89  1.    0.455 0.919 0.98 ]\n",
      "val_sens 0.780+/-0.186 : [0.9   0.909 0.417 0.8   0.875]\n",
      "val_spec 0.827+/-0.172 : [0.9   1.    0.5   0.9   0.833]\n",
      "val_prec 0.824+/-0.152 : [0.9   1.    0.556 0.889 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31]\n",
      "train_accuracy 0.760+/-0.182 : [0.65  0.988 0.512 0.7   0.95 ]\n",
      "val_accuracy 0.550+/-0.158 : [0.5  0.65 0.45 0.35 0.8 ]\n",
      "val_auc 0.569+/-0.225 : [0.61  0.74  0.343 0.283 0.869]\n",
      "val_sens 0.462+/-0.257 : [0.5   0.667 0.417 0.    0.727]\n",
      "val_spec 0.593+/-0.162 : [0.5   0.636 0.5   0.438 0.889]\n",
      "val_prec 0.489+/-0.291 : [0.4   0.6   0.556 0.    0.889]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704eaf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e944e6d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "6  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136]\n",
      "train_accuracy 0.933+/-0.138 : [0.988 1.    0.625 0.988 1.    1.   ]\n",
      "val_accuracy 0.817+/-0.167 : [0.9  0.95 0.45 0.85 0.85 0.9 ]\n",
      "val_auc 0.867+/-0.188 : [0.89  1.    0.455 0.919 0.98  0.96 ]\n",
      "val_sens 0.800+/-0.175 : [0.9   0.909 0.417 0.8   0.875 0.9  ]\n",
      "val_spec 0.839+/-0.159 : [0.9   1.    0.5   0.9   0.833 0.9  ]\n",
      "val_prec 0.837+/-0.141 : [0.9   1.    0.556 0.889 0.778 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18]\n",
      "train_accuracy 0.771+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825]\n",
      "val_accuracy 0.575+/-0.155 : [0.5  0.65 0.45 0.35 0.8  0.7 ]\n",
      "val_auc 0.581+/-0.207 : [0.61  0.74  0.343 0.283 0.869 0.64 ]\n",
      "val_sens 0.510+/-0.258 : [0.5   0.667 0.417 0.    0.727 0.75 ]\n",
      "val_spec 0.605+/-0.150 : [0.5   0.636 0.5   0.438 0.889 0.667]\n",
      "val_prec 0.507+/-0.269 : [0.4   0.6   0.556 0.    0.889 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9425cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9438b158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "7  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967]\n",
      "train_accuracy 0.943+/-0.130 : [0.988 1.    0.625 0.988 1.    1.    1.   ]\n",
      "val_accuracy 0.829+/-0.158 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9 ]\n",
      "val_auc 0.886+/-0.180 : [0.89  1.    0.455 0.919 0.98  0.96  1.   ]\n",
      "val_sens 0.829+/-0.177 : [0.9   0.909 0.417 0.8   0.875 0.9   1.   ]\n",
      "val_spec 0.838+/-0.147 : [0.9   1.    0.5   0.9   0.833 0.9   0.833]\n",
      "val_prec 0.832+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29]\n",
      "train_accuracy 0.802+/-0.173 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988]\n",
      "val_accuracy 0.579+/-0.144 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6 ]\n",
      "val_auc 0.611+/-0.205 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79 ]\n",
      "val_sens 0.527+/-0.242 : [0.5   0.667 0.417 0.    0.727 0.75  0.625]\n",
      "val_spec 0.602+/-0.139 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583]\n",
      "val_prec 0.506+/-0.249 : [0.4   0.6   0.556 0.    0.889 0.6   0.5  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed8041158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5057a598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "8  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35]\n",
      "train_accuracy 0.948+/-0.122 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988]\n",
      "val_accuracy 0.812+/-0.154 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7 ]\n",
      "val_auc 0.861+/-0.181 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687]\n",
      "val_sens 0.805+/-0.177 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636]\n",
      "val_spec 0.831+/-0.139 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778]\n",
      "val_prec 0.825+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1]\n",
      "train_accuracy 0.769+/-0.183 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54 ]\n",
      "val_accuracy 0.600+/-0.146 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75]\n",
      "val_auc 0.635+/-0.203 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808]\n",
      "val_sens 0.565+/-0.248 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833]\n",
      "val_spec 0.616+/-0.136 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714]\n",
      "val_prec 0.513+/-0.233 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947e9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50513510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "9  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72]\n",
      "train_accuracy 0.954+/-0.117 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.   ]\n",
      "val_accuracy 0.828+/-0.151 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95]\n",
      "val_auc 0.871+/-0.173 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949]\n",
      "val_sens 0.815+/-0.170 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9  ]\n",
      "val_spec 0.849+/-0.142 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.   ]\n",
      "val_prec 0.844+/-0.129 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4]\n",
      "train_accuracy 0.754+/-0.178 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637]\n",
      "val_accuracy 0.611+/-0.141 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7 ]\n",
      "val_auc 0.643+/-0.193 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707]\n",
      "val_sens 0.576+/-0.236 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667]\n",
      "val_spec 0.628+/-0.132 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727]\n",
      "val_prec 0.530+/-0.225 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0652598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9434ed08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "10  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600]\n",
      "train_accuracy 0.959+/-0.111 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.   ]\n",
      "val_accuracy 0.835+/-0.145 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9 ]\n",
      "val_auc 0.883+/-0.168 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99 ]\n",
      "val_sens 0.823+/-0.162 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889]\n",
      "val_spec 0.855+/-0.136 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909]\n",
      "val_prec 0.849+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42]\n",
      "train_accuracy 0.778+/-0.182 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988]\n",
      "val_accuracy 0.625+/-0.140 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75]\n",
      "val_auc 0.664+/-0.193 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848]\n",
      "val_sens 0.585+/-0.226 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667]\n",
      "val_spec 0.653+/-0.146 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875]\n",
      "val_prec 0.566+/-0.239 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee02366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac50a400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "11  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36]\n",
      "train_accuracy 0.952+/-0.108 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887]\n",
      "val_accuracy 0.823+/-0.144 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7 ]\n",
      "val_auc 0.879+/-0.161 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84 ]\n",
      "val_sens 0.816+/-0.156 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75 ]\n",
      "val_spec 0.838+/-0.140 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667]\n",
      "val_prec 0.826+/-0.138 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21]\n",
      "train_accuracy 0.791+/-0.179 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925]\n",
      "val_accuracy 0.632+/-0.135 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7 ]\n",
      "val_auc 0.677+/-0.189 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81 ]\n",
      "val_sens 0.600+/-0.220 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75 ]\n",
      "val_spec 0.654+/-0.139 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667]\n",
      "val_prec 0.569+/-0.229 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee066dc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee280d048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "12  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361]\n",
      "train_accuracy 0.956+/-0.104 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.   ]\n",
      "val_accuracy 0.829+/-0.139 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9 ]\n",
      "val_auc 0.886+/-0.155 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96 ]\n",
      "val_sens 0.831+/-0.158 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.   ]\n",
      "val_spec 0.838+/-0.134 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833]\n",
      "val_prec 0.824+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69]\n",
      "train_accuracy 0.809+/-0.181 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.   ]\n",
      "val_accuracy 0.642+/-0.134 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75]\n",
      "val_auc 0.696+/-0.191 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9  ]\n",
      "val_sens 0.611+/-0.214 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727]\n",
      "val_spec 0.664+/-0.138 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778]\n",
      "val_prec 0.588+/-0.228 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee2802510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94367bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "13  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34]\n",
      "train_accuracy 0.957+/-0.100 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962]\n",
      "val_accuracy 0.815+/-0.142 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65]\n",
      "val_auc 0.874+/-0.155 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727]\n",
      "val_sens 0.814+/-0.164 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6  ]\n",
      "val_spec 0.827+/-0.134 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7  ]\n",
      "val_prec 0.812+/-0.134 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4]\n",
      "train_accuracy 0.788+/-0.188 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538]\n",
      "val_accuracy 0.627+/-0.138 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45]\n",
      "val_auc 0.669+/-0.205 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354]\n",
      "val_sens 0.583+/-0.227 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25 ]\n",
      "val_spec 0.652+/-0.139 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5  ]\n",
      "val_prec 0.551+/-0.253 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3043d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704ea620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "14  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47]\n",
      "train_accuracy 0.957+/-0.097 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962]\n",
      "val_accuracy 0.811+/-0.138 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75]\n",
      "val_auc 0.868+/-0.151 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798]\n",
      "val_sens 0.805+/-0.161 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7  ]\n",
      "val_spec 0.825+/-0.129 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8  ]\n",
      "val_prec 0.810+/-0.129 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10]\n",
      "train_accuracy 0.788+/-0.181 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788]\n",
      "val_accuracy 0.632+/-0.134 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7 ]\n",
      "val_auc 0.661+/-0.200 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545]\n",
      "val_sens 0.585+/-0.219 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615]\n",
      "val_spec 0.666+/-0.144 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857]\n",
      "val_prec 0.575+/-0.259 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70181268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed80b4378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "15  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797]\n",
      "train_accuracy 0.960+/-0.094 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.   ]\n",
      "val_accuracy 0.820+/-0.138 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95]\n",
      "val_auc 0.877+/-0.149 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.   ]\n",
      "val_sens 0.818+/-0.163 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.   ]\n",
      "val_spec 0.831+/-0.127 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917]\n",
      "val_prec 0.815+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36]\n",
      "train_accuracy 0.801+/-0.182 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988]\n",
      "val_accuracy 0.640+/-0.133 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75]\n",
      "val_auc 0.674+/-0.200 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869]\n",
      "val_sens 0.593+/-0.213 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7  ]\n",
      "val_spec 0.675+/-0.143 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8  ]\n",
      "val_prec 0.589+/-0.255 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e944fd840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30511730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "16  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62]\n",
      "train_accuracy 0.962+/-0.091 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988]\n",
      "val_accuracy 0.819+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8 ]\n",
      "val_auc 0.875+/-0.145 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85 ]\n",
      "val_sens 0.817+/-0.157 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8  ]\n",
      "val_spec 0.829+/-0.123 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8  ]\n",
      "val_prec 0.814+/-0.122 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9]\n",
      "train_accuracy 0.800+/-0.176 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788]\n",
      "val_accuracy 0.631+/-0.133 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5 ]\n",
      "val_auc 0.672+/-0.194 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64 ]\n",
      "val_sens 0.587+/-0.208 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5  ]\n",
      "val_spec 0.664+/-0.145 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5  ]\n",
      "val_prec 0.577+/-0.251 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee06527b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e503df048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469]\n",
      "train_accuracy 0.964+/-0.089 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.   ]\n",
      "val_accuracy 0.829+/-0.136 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.  ]\n",
      "val_auc 0.883+/-0.143 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.   ]\n",
      "val_sens 0.828+/-0.159 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.   ]\n",
      "val_spec 0.839+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.   ]\n",
      "val_prec 0.825+/-0.127 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38]\n",
      "train_accuracy 0.811+/-0.177 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988]\n",
      "val_accuracy 0.629+/-0.130 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6 ]\n",
      "val_auc 0.676+/-0.189 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74 ]\n",
      "val_sens 0.589+/-0.202 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625]\n",
      "val_spec 0.660+/-0.142 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583]\n",
      "val_prec 0.573+/-0.245 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50774ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94272a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "18  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26]\n",
      "train_accuracy 0.962+/-0.087 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938]\n",
      "val_accuracy 0.819+/-0.139 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65]\n",
      "val_auc 0.876+/-0.142 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768]\n",
      "val_sens 0.814+/-0.164 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583]\n",
      "val_spec 0.834+/-0.124 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75 ]\n",
      "val_prec 0.822+/-0.123 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1]\n",
      "train_accuracy 0.797+/-0.182 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55 ]\n",
      "val_accuracy 0.619+/-0.132 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45]\n",
      "val_auc 0.667+/-0.187 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515]\n",
      "val_sens 0.577+/-0.202 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375]\n",
      "val_spec 0.651+/-0.143 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5  ]\n",
      "val_prec 0.559+/-0.244 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e7046e7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac3a1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "19  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79]\n",
      "train_accuracy 0.964+/-0.085 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.   ]\n",
      "val_accuracy 0.818+/-0.135 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8 ]\n",
      "val_auc 0.880+/-0.139 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939]\n",
      "val_sens 0.810+/-0.161 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727]\n",
      "val_spec 0.837+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889]\n",
      "val_prec 0.826+/-0.121 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21]\n",
      "train_accuracy 0.801+/-0.178 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887]\n",
      "val_accuracy 0.621+/-0.129 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65]\n",
      "val_auc 0.670+/-0.182 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727]\n",
      "val_sens 0.580+/-0.197 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625]\n",
      "val_spec 0.652+/-0.139 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667]\n",
      "val_prec 0.559+/-0.237 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0408b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30319950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "20  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423]\n",
      "train_accuracy 0.966+/-0.083 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.   ]\n",
      "val_accuracy 0.823+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9 ]\n",
      "val_auc 0.885+/-0.137 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99 ]\n",
      "val_sens 0.814+/-0.158 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889]\n",
      "val_spec 0.841+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909]\n",
      "val_prec 0.829+/-0.119 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41]\n",
      "train_accuracy 0.811+/-0.179 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.   ]\n",
      "val_accuracy 0.630+/-0.132 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8 ]\n",
      "val_auc 0.683+/-0.187 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929]\n",
      "val_sens 0.587+/-0.195 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727]\n",
      "val_spec 0.663+/-0.145 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889]\n",
      "val_prec 0.576+/-0.242 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee03f17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e705621e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "21  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53]\n",
      "train_accuracy 0.964+/-0.081 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925]\n",
      "val_accuracy 0.819+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75]\n",
      "val_auc 0.884+/-0.134 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87 ]\n",
      "val_sens 0.812+/-0.154 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778]\n",
      "val_spec 0.835+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727]\n",
      "val_prec 0.823+/-0.119 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5]\n",
      "train_accuracy 0.807+/-0.176 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712]\n",
      "val_accuracy 0.633+/-0.129 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7 ]\n",
      "val_auc 0.684+/-0.182 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69 ]\n",
      "val_sens 0.595+/-0.193 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75 ]\n",
      "val_spec 0.664+/-0.142 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667]\n",
      "val_prec 0.577+/-0.236 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704ea048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9417d840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "22  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100]\n",
      "train_accuracy 0.966+/-0.080 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.   ]\n",
      "val_accuracy 0.820+/-0.128 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85]\n",
      "val_auc 0.889+/-0.133 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98 ]\n",
      "val_sens 0.812+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818]\n",
      "val_spec 0.838+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889]\n",
      "val_prec 0.826+/-0.118 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18]\n",
      "train_accuracy 0.811+/-0.173 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913]\n",
      "val_accuracy 0.634+/-0.126 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65]\n",
      "val_auc 0.688+/-0.179 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77 ]\n",
      "val_sens 0.598+/-0.189 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667]\n",
      "val_spec 0.662+/-0.138 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636]\n",
      "val_prec 0.578+/-0.231 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9477cea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0236ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "23  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21]\n",
      "train_accuracy 0.967+/-0.078 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988]\n",
      "val_accuracy 0.811+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6 ]\n",
      "val_auc 0.883+/-0.133 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747]\n",
      "val_sens 0.800+/-0.158 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538]\n",
      "val_spec 0.833+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714]\n",
      "val_prec 0.824+/-0.115 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3]\n",
      "train_accuracy 0.803+/-0.174 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625]\n",
      "val_accuracy 0.628+/-0.127 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5 ]\n",
      "val_auc 0.681+/-0.178 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535]\n",
      "val_sens 0.592+/-0.187 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462]\n",
      "val_spec 0.658+/-0.137 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571]\n",
      "val_prec 0.582+/-0.227 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac665048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e945d9730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67]\n",
      "train_accuracy 0.967+/-0.077 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962]\n",
      "val_accuracy 0.810+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8 ]\n",
      "val_auc 0.880+/-0.130 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828]\n",
      "val_sens 0.797+/-0.155 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727]\n",
      "val_spec 0.835+/-0.115 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889]\n",
      "val_prec 0.827+/-0.114 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10]\n",
      "train_accuracy 0.801+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75 ]\n",
      "val_accuracy 0.627+/-0.124 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6 ]\n",
      "val_auc 0.680+/-0.174 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646]\n",
      "val_sens 0.590+/-0.183 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545]\n",
      "val_spec 0.659+/-0.134 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667]\n",
      "val_prec 0.585+/-0.223 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50172f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70263400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "25  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000]\n",
      "train_accuracy 0.968+/-0.075 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.   ]\n",
      "val_accuracy 0.818+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.  ]\n",
      "val_auc 0.885+/-0.130 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.   ]\n",
      "val_sens 0.805+/-0.157 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.   ]\n",
      "val_spec 0.842+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.   ]\n",
      "val_prec 0.834+/-0.116 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26]\n",
      "train_accuracy 0.808+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962]\n",
      "val_accuracy 0.626+/-0.122 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6 ]\n",
      "val_auc 0.681+/-0.171 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727]\n",
      "val_sens 0.589+/-0.180 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545]\n",
      "val_spec 0.659+/-0.131 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667]\n",
      "val_prec 0.588+/-0.219 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e946a9950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e701c5510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "26  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44]\n",
      "train_accuracy 0.968+/-0.074 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962]\n",
      "val_accuracy 0.815+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75]\n",
      "val_auc 0.885+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87 ]\n",
      "val_sens 0.804+/-0.154 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778]\n",
      "val_spec 0.837+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727]\n",
      "val_prec 0.829+/-0.117 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19]\n",
      "train_accuracy 0.812+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925]\n",
      "val_accuracy 0.627+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65]\n",
      "val_auc 0.684+/-0.168 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75 ]\n",
      "val_sens 0.592+/-0.177 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667]\n",
      "val_spec 0.658+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636]\n",
      "val_prec 0.589+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0391620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e946bd0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "27  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184]\n",
      "train_accuracy 0.969+/-0.073 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.   ]\n",
      "val_accuracy 0.820+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95]\n",
      "val_auc 0.887+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94 ]\n",
      "val_sens 0.812+/-0.156 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.   ]\n",
      "val_spec 0.840+/-0.116 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909]\n",
      "val_prec 0.831+/-0.116 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60]\n",
      "train_accuracy 0.819+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.   ]\n",
      "val_accuracy 0.630+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7 ]\n",
      "val_auc 0.690+/-0.168 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85 ]\n",
      "val_sens 0.596+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7  ]\n",
      "val_spec 0.660+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7  ]\n",
      "val_prec 0.593+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e701c5bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac10ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "28  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13]\n",
      "train_accuracy 0.967+/-0.072 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913]\n",
      "val_accuracy 0.809+/-0.141 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5 ]\n",
      "val_auc 0.878+/-0.132 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636]\n",
      "val_sens 0.798+/-0.168 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429]\n",
      "val_spec 0.829+/-0.127 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538]\n",
      "val_prec 0.813+/-0.146 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2]\n",
      "train_accuracy 0.809+/-0.174 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525]\n",
      "val_accuracy 0.623+/-0.121 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45]\n",
      "val_auc 0.684+/-0.168 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505]\n",
      "val_sens 0.589+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417]\n",
      "val_spec 0.654+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5  ]\n",
      "val_prec 0.592+/-0.208 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e502f87b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70549510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "29  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42]\n",
      "train_accuracy 0.967+/-0.071 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962]\n",
      "val_accuracy 0.812+/-0.140 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9 ]\n",
      "val_auc 0.878+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879]\n",
      "val_sens 0.799+/-0.166 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818]\n",
      "val_spec 0.835+/-0.128 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.   ]\n",
      "val_prec 0.820+/-0.148 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14]\n",
      "train_accuracy 0.808+/-0.171 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8  ]\n",
      "val_accuracy 0.624+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65]\n",
      "val_auc 0.681+/-0.166 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606]\n",
      "val_sens 0.592+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667]\n",
      "val_spec 0.654+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643]\n",
      "val_prec 0.587+/-0.206 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94367bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9477cae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "30  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491]\n",
      "train_accuracy 0.968+/-0.070 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.   ]\n",
      "val_accuracy 0.818+/-0.142 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.  ]\n",
      "val_auc 0.882+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.   ]\n",
      "val_sens 0.805+/-0.167 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.   ]\n",
      "val_spec 0.840+/-0.130 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.   ]\n",
      "val_prec 0.826+/-0.149 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30]\n",
      "train_accuracy 0.814+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988]\n",
      "val_accuracy 0.627+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7 ]\n",
      "val_auc 0.684+/-0.164 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768]\n",
      "val_sens 0.593+/-0.170 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636]\n",
      "val_spec 0.658+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778]\n",
      "val_prec 0.593+/-0.205 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac07b730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e945ec510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "31  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34]\n",
      "train_accuracy 0.967+/-0.069 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938]\n",
      "val_accuracy 0.816+/-0.140 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75]\n",
      "val_auc 0.878+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78 ]\n",
      "val_sens 0.805+/-0.164 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778]\n",
      "val_spec 0.837+/-0.129 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727]\n",
      "val_prec 0.822+/-0.148 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11]\n",
      "train_accuracy 0.815+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825]\n",
      "val_accuracy 0.629+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7 ]\n",
      "val_auc 0.681+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6  ]\n",
      "val_sens 0.598+/-0.169 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75 ]\n",
      "val_spec 0.658+/-0.123 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667]\n",
      "val_prec 0.593+/-0.202 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac2922f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94703510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "32  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397]\n",
      "train_accuracy 0.968+/-0.068 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.   ]\n",
      "val_accuracy 0.819+/-0.138 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9 ]\n",
      "val_auc 0.882+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99 ]\n",
      "val_sens 0.807+/-0.162 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9  ]\n",
      "val_spec 0.839+/-0.128 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9  ]\n",
      "val_prec 0.824+/-0.146 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24]\n",
      "train_accuracy 0.819+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962]\n",
      "val_accuracy 0.627+/-0.115 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55]\n",
      "val_auc 0.682+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7  ]\n",
      "val_sens 0.597+/-0.167 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556]\n",
      "val_spec 0.655+/-0.123 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545]\n",
      "val_prec 0.590+/-0.200 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e504b7c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac3a1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "33  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25]\n",
      "train_accuracy 0.967+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95 ]\n",
      "val_accuracy 0.812+/-0.141 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6 ]\n",
      "val_auc 0.878+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737]\n",
      "val_sens 0.799+/-0.166 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538]\n",
      "val_spec 0.835+/-0.127 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714]\n",
      "val_prec 0.823+/-0.144 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1]\n",
      "train_accuracy 0.812+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59 ]\n",
      "val_accuracy 0.623+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5 ]\n",
      "val_auc 0.671+/-0.168 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333]\n",
      "val_sens 0.592+/-0.167 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429]\n",
      "val_spec 0.651+/-0.123 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538]\n",
      "val_prec 0.582+/-0.201 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0632950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0632048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "34  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70]\n",
      "train_accuracy 0.968+/-0.066 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975]\n",
      "val_accuracy 0.812+/-0.139 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8 ]\n",
      "val_auc 0.878+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899]\n",
      "val_sens 0.799+/-0.164 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778]\n",
      "val_spec 0.834+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818]\n",
      "val_prec 0.822+/-0.142 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22]\n",
      "train_accuracy 0.814+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887]\n",
      "val_accuracy 0.624+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65]\n",
      "val_auc 0.675+/-0.167 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798]\n",
      "val_sens 0.593+/-0.164 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625]\n",
      "val_spec 0.652+/-0.121 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667]\n",
      "val_prec 0.582+/-0.198 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac44e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac0e3ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "35  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164]\n",
      "train_accuracy 0.968+/-0.065 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988]\n",
      "val_accuracy 0.813+/-0.137 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85]\n",
      "val_auc 0.881+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98 ]\n",
      "val_sens 0.797+/-0.162 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75 ]\n",
      "val_spec 0.839+/-0.127 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.   ]\n",
      "val_prec 0.827+/-0.144 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28]\n",
      "train_accuracy 0.817+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913]\n",
      "val_accuracy 0.627+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75]\n",
      "val_auc 0.679+/-0.166 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808]\n",
      "val_sens 0.597+/-0.164 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75 ]\n",
      "val_spec 0.654+/-0.120 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75 ]\n",
      "val_prec 0.584+/-0.196 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac210ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e501e8598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "36  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43]\n",
      "train_accuracy 0.967+/-0.065 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925]\n",
      "val_accuracy 0.812+/-0.136 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8 ]\n",
      "val_auc 0.881+/-0.124 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88 ]\n",
      "val_sens 0.797+/-0.160 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8  ]\n",
      "val_spec 0.838+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8  ]\n",
      "val_prec 0.826+/-0.142 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11]\n",
      "train_accuracy 0.818+/-0.164 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837]\n",
      "val_accuracy 0.628+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65]\n",
      "val_auc 0.680+/-0.164 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72 ]\n",
      "val_sens 0.599+/-0.162 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667]\n",
      "val_spec 0.654+/-0.119 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636]\n",
      "val_prec 0.585+/-0.193 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704eaae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed80417b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "37  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477]\n",
      "train_accuracy 0.968+/-0.064 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.   ]\n",
      "val_accuracy 0.818+/-0.137 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.  ]\n",
      "val_auc 0.884+/-0.124 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.   ]\n",
      "val_sens 0.803+/-0.161 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.   ]\n",
      "val_spec 0.842+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.   ]\n",
      "val_prec 0.831+/-0.143 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37]\n",
      "train_accuracy 0.823+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.   ]\n",
      "val_accuracy 0.630+/-0.112 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7 ]\n",
      "val_auc 0.683+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78 ]\n",
      "val_sens 0.606+/-0.164 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833]\n",
      "val_spec 0.654+/-0.117 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643]\n",
      "val_prec 0.582+/-0.191 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e302f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e305560d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "38  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33]\n",
      "train_accuracy 0.967+/-0.064 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95 ]\n",
      "val_accuracy 0.814+/-0.137 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7 ]\n",
      "val_auc 0.881+/-0.124 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758]\n",
      "val_sens 0.798+/-0.161 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636]\n",
      "val_spec 0.841+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778]\n",
      "val_prec 0.829+/-0.141 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3]\n",
      "train_accuracy 0.817+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613]\n",
      "val_accuracy 0.630+/-0.110 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65]\n",
      "val_auc 0.680+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576]\n",
      "val_sens 0.606+/-0.162 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6  ]\n",
      "val_spec 0.655+/-0.116 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7  ]\n",
      "val_prec 0.585+/-0.189 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e304dcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0408598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "39  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53]\n",
      "train_accuracy 0.968+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988]\n",
      "val_accuracy 0.814+/-0.135 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8 ]\n",
      "val_auc 0.881+/-0.122 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879]\n",
      "val_sens 0.798+/-0.159 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778]\n",
      "val_spec 0.840+/-0.123 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818]\n",
      "val_prec 0.828+/-0.139 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7]\n",
      "train_accuracy 0.814+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688]\n",
      "val_accuracy 0.626+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45]\n",
      "val_auc 0.676+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535]\n",
      "val_sens 0.596+/-0.170 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25 ]\n",
      "val_spec 0.651+/-0.117 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5  ]\n",
      "val_prec 0.572+/-0.201 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947031e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e503329d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "40  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85]\n",
      "train_accuracy 0.968+/-0.062 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988]\n",
      "val_accuracy 0.816+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9 ]\n",
      "val_auc 0.883+/-0.121 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949]\n",
      "val_sens 0.800+/-0.157 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889]\n",
      "val_spec 0.842+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909]\n",
      "val_prec 0.829+/-0.138 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29]\n",
      "train_accuracy 0.818+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962]\n",
      "val_accuracy 0.626+/-0.111 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65]\n",
      "val_auc 0.680+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838]\n",
      "val_sens 0.596+/-0.168 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583]\n",
      "val_spec 0.653+/-0.116 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75 ]\n",
      "val_prec 0.577+/-0.201 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee06ef488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed82d7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "41  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84]\n",
      "train_accuracy 0.969+/-0.061 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.   ]\n",
      "val_accuracy 0.817+/-0.132 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85]\n",
      "val_auc 0.885+/-0.120 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96 ]\n",
      "val_sens 0.801+/-0.156 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818]\n",
      "val_spec 0.843+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889]\n",
      "val_prec 0.831+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19]\n",
      "train_accuracy 0.820+/-0.163 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9  ]\n",
      "val_accuracy 0.627+/-0.110 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65]\n",
      "val_auc 0.682+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75 ]\n",
      "val_sens 0.598+/-0.166 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667]\n",
      "val_spec 0.653+/-0.115 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636]\n",
      "val_prec 0.578+/-0.199 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e506c8730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac45d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "42  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344]\n",
      "train_accuracy 0.970+/-0.061 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.   ]\n",
      "val_accuracy 0.819+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9 ]\n",
      "val_auc 0.886+/-0.119 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96 ]\n",
      "val_sens 0.801+/-0.154 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833]\n",
      "val_spec 0.847+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.   ]\n",
      "val_prec 0.835+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19]\n",
      "train_accuracy 0.822+/-0.162 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938]\n",
      "val_accuracy 0.626+/-0.109 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6 ]\n",
      "val_auc 0.683+/-0.157 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72 ]\n",
      "val_sens 0.598+/-0.164 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6  ]\n",
      "val_spec 0.652+/-0.114 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6  ]\n",
      "val_prec 0.579+/-0.196 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70414488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed82d79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "43  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29]\n",
      "train_accuracy 0.969+/-0.061 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925]\n",
      "val_accuracy 0.815+/-0.132 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65]\n",
      "val_auc 0.880+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596]\n",
      "val_sens 0.796+/-0.156 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583]\n",
      "val_spec 0.845+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75 ]\n",
      "val_prec 0.834+/-0.136 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4]\n",
      "train_accuracy 0.816+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55 ]\n",
      "val_accuracy 0.624+/-0.108 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55]\n",
      "val_auc 0.676+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384]\n",
      "val_sens 0.596+/-0.163 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5  ]\n",
      "val_spec 0.650+/-0.113 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583]\n",
      "val_prec 0.575+/-0.195 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947f3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3028c510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "44  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79]\n",
      "train_accuracy 0.969+/-0.060 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988]\n",
      "val_accuracy 0.816+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85]\n",
      "val_auc 0.881+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96 ]\n",
      "val_sens 0.795+/-0.154 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75 ]\n",
      "val_spec 0.848+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.   ]\n",
      "val_prec 0.838+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16]\n",
      "train_accuracy 0.819+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95 ]\n",
      "val_accuracy 0.622+/-0.108 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5 ]\n",
      "val_auc 0.672+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525]\n",
      "val_sens 0.591+/-0.163 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4  ]\n",
      "val_spec 0.647+/-0.113 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533]\n",
      "val_prec 0.567+/-0.200 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947f3730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac38e510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "45  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000]\n",
      "train_accuracy 0.970+/-0.059 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.   ]\n",
      "val_accuracy 0.820+/-0.132 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.  ]\n",
      "val_auc 0.884+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.   ]\n",
      "val_sens 0.800+/-0.155 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.   ]\n",
      "val_spec 0.851+/-0.123 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.   ]\n",
      "val_prec 0.841+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34]\n",
      "train_accuracy 0.823+/-0.165 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.   ]\n",
      "val_accuracy 0.624+/-0.109 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75]\n",
      "val_auc 0.677+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889]\n",
      "val_sens 0.593+/-0.162 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667]\n",
      "val_spec 0.652+/-0.117 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875]\n",
      "val_prec 0.575+/-0.203 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee01af268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e705a6840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "46  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72]\n",
      "train_accuracy 0.970+/-0.059 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988]\n",
      "val_accuracy 0.820+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8 ]\n",
      "val_auc 0.885+/-0.123 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91 ]\n",
      "val_sens 0.800+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8  ]\n",
      "val_spec 0.850+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8  ]\n",
      "val_prec 0.840+/-0.136 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17]\n",
      "train_accuracy 0.824+/-0.163 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863]\n",
      "val_accuracy 0.625+/-0.108 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65]\n",
      "val_auc 0.677+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67 ]\n",
      "val_sens 0.594+/-0.161 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667]\n",
      "val_spec 0.652+/-0.115 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636]\n",
      "val_prec 0.575+/-0.201 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee060be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee02366a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "47  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101]\n",
      "train_accuracy 0.971+/-0.058 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.   ]\n",
      "val_accuracy 0.820+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85]\n",
      "val_auc 0.886+/-0.122 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93 ]\n",
      "val_sens 0.800+/-0.152 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818]\n",
      "val_spec 0.851+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889]\n",
      "val_prec 0.842+/-0.135 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51]\n",
      "train_accuracy 0.827+/-0.163 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988]\n",
      "val_accuracy 0.628+/-0.108 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75]\n",
      "val_auc 0.679+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79 ]\n",
      "val_sens 0.597+/-0.160 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727]\n",
      "val_spec 0.655+/-0.116 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778]\n",
      "val_prec 0.580+/-0.202 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e500e0ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3011e048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "48  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32]\n",
      "train_accuracy 0.971+/-0.058 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975]\n",
      "val_accuracy 0.819+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75]\n",
      "val_auc 0.884+/-0.122 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788]\n",
      "val_sens 0.798+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7  ]\n",
      "val_spec 0.850+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8  ]\n",
      "val_prec 0.840+/-0.134 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3]\n",
      "train_accuracy 0.824+/-0.163 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675]\n",
      "val_accuracy 0.624+/-0.110 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45]\n",
      "val_auc 0.673+/-0.164 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394]\n",
      "val_sens 0.594+/-0.160 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417]\n",
      "val_spec 0.652+/-0.117 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5  ]\n",
      "val_prec 0.579+/-0.200 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94272a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0207510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "49  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75]\n",
      "train_accuracy 0.972+/-0.057 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.   ]\n",
      "val_accuracy 0.819+/-0.127 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85]\n",
      "val_auc 0.884+/-0.121 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909]\n",
      "val_sens 0.800+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875]\n",
      "val_spec 0.850+/-0.118 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833]\n",
      "val_prec 0.839+/-0.133 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4]\n",
      "train_accuracy 0.820+/-0.164 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613]\n",
      "val_accuracy 0.620+/-0.112 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45]\n",
      "val_auc 0.672+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586]\n",
      "val_sens 0.588+/-0.163 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333]\n",
      "val_spec 0.648+/-0.117 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5  ]\n",
      "val_prec 0.572+/-0.204 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30510c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947f32f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "50  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495]\n",
      "train_accuracy 0.972+/-0.057 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.   ]\n",
      "val_accuracy 0.822+/-0.127 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95]\n",
      "val_auc 0.886+/-0.120 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96 ]\n",
      "val_sens 0.802+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9  ]\n",
      "val_spec 0.853+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.   ]\n",
      "val_prec 0.842+/-0.133 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47]\n",
      "train_accuracy 0.823+/-0.164 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988]\n",
      "val_accuracy 0.625+/-0.115 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85]\n",
      "val_auc 0.676+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879]\n",
      "val_sens 0.591+/-0.163 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75 ]\n",
      "val_spec 0.655+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.   ]\n",
      "val_prec 0.581+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.   ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e301fd268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee066df28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "51  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61]\n",
      "train_accuracy 0.973+/-0.056 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.   ]\n",
      "val_accuracy 0.822+/-0.126 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8 ]\n",
      "val_auc 0.885+/-0.119 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87 ]\n",
      "val_sens 0.802+/-0.147 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8  ]\n",
      "val_spec 0.852+/-0.118 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8  ]\n",
      "val_prec 0.841+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2]\n",
      "train_accuracy 0.818+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575]\n",
      "val_accuracy 0.623+/-0.115 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5 ]\n",
      "val_auc 0.673+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52 ]\n",
      "val_sens 0.590+/-0.162 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5  ]\n",
      "val_spec 0.652+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5  ]\n",
      "val_prec 0.577+/-0.210 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e503ebc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e505d5598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "52  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182]\n",
      "train_accuracy 0.973+/-0.056 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.   ]\n",
      "val_accuracy 0.824+/-0.126 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95]\n",
      "val_auc 0.887+/-0.118 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97 ]\n",
      "val_sens 0.806+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.   ]\n",
      "val_spec 0.853+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909]\n",
      "val_prec 0.843+/-0.131 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24]\n",
      "train_accuracy 0.821+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975]\n",
      "val_accuracy 0.623+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65]\n",
      "val_auc 0.674+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75 ]\n",
      "val_sens 0.592+/-0.161 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714]\n",
      "val_spec 0.652+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615]\n",
      "val_prec 0.576+/-0.208 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac7d4840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94272268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "53  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31]\n",
      "train_accuracy 0.973+/-0.056 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962]\n",
      "val_accuracy 0.820+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6 ]\n",
      "val_auc 0.884+/-0.119 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717]\n",
      "val_sens 0.801+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545]\n",
      "val_spec 0.849+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667]\n",
      "val_prec 0.839+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2]\n",
      "train_accuracy 0.815+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463]\n",
      "val_accuracy 0.623+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6 ]\n",
      "val_auc 0.671+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525]\n",
      "val_sens 0.591+/-0.160 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556]\n",
      "val_spec 0.651+/-0.124 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636]\n",
      "val_prec 0.575+/-0.206 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac675048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e700e0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "54  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58]\n",
      "train_accuracy 0.974+/-0.055 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.   ]\n",
      "val_accuracy 0.819+/-0.127 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8 ]\n",
      "val_auc 0.885+/-0.118 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929]\n",
      "val_sens 0.800+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778]\n",
      "val_spec 0.849+/-0.118 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818]\n",
      "val_prec 0.838+/-0.131 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17]\n",
      "train_accuracy 0.816+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887]\n",
      "val_accuracy 0.623+/-0.112 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65]\n",
      "val_auc 0.673+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778]\n",
      "val_sens 0.593+/-0.159 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667]\n",
      "val_spec 0.651+/-0.123 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643]\n",
      "val_prec 0.573+/-0.205 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70585488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50677510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "55  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171]\n",
      "train_accuracy 0.974+/-0.055 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.   ]\n",
      "val_accuracy 0.821+/-0.127 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9 ]\n",
      "val_auc 0.886+/-0.118 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949]\n",
      "val_sens 0.802+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889]\n",
      "val_spec 0.850+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909]\n",
      "val_prec 0.839+/-0.130 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42]\n",
      "train_accuracy 0.819+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988]\n",
      "val_accuracy 0.627+/-0.115 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85]\n",
      "val_auc 0.678+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929]\n",
      "val_sens 0.598+/-0.162 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875]\n",
      "val_spec 0.655+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833]\n",
      "val_prec 0.577+/-0.205 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e302dcd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e502f9510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "56  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81]\n",
      "train_accuracy 0.975+/-0.054 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.   ]\n",
      "val_accuracy 0.822+/-0.126 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9 ]\n",
      "val_auc 0.887+/-0.117 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95 ]\n",
      "val_sens 0.804+/-0.148 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9  ]\n",
      "val_spec 0.851+/-0.116 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9  ]\n",
      "val_prec 0.840+/-0.129 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15]\n",
      "train_accuracy 0.820+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85 ]\n",
      "val_accuracy 0.628+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65]\n",
      "val_auc 0.680+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77 ]\n",
      "val_sens 0.599+/-0.160 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636]\n",
      "val_spec 0.655+/-0.123 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667]\n",
      "val_prec 0.579+/-0.204 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50640378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ef138e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "57  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444]\n",
      "train_accuracy 0.975+/-0.054 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.   ]\n",
      "val_accuracy 0.825+/-0.126 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95]\n",
      "val_auc 0.889+/-0.117 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.   ]\n",
      "val_sens 0.805+/-0.148 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909]\n",
      "val_spec 0.853+/-0.117 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.   ]\n",
      "val_prec 0.843+/-0.130 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28]\n",
      "train_accuracy 0.822+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962]\n",
      "val_accuracy 0.628+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65]\n",
      "val_auc 0.681+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73 ]\n",
      "val_sens 0.600+/-0.159 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667]\n",
      "val_spec 0.654+/-0.122 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636]\n",
      "val_prec 0.579+/-0.202 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac434598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee27aabf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15]\n",
      "train_accuracy 0.974+/-0.054 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925]\n",
      "val_accuracy 0.823+/-0.125 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75]\n",
      "val_auc 0.886+/-0.118 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727]\n",
      "val_sens 0.804+/-0.147 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7  ]\n",
      "val_spec 0.852+/-0.116 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8  ]\n",
      "val_prec 0.842+/-0.129 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2]\n",
      "train_accuracy 0.818+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587]\n",
      "val_accuracy 0.627+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55]\n",
      "val_auc 0.677+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495]\n",
      "val_sens 0.598+/-0.158 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5  ]\n",
      "val_spec 0.654+/-0.122 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6  ]\n",
      "val_prec 0.579+/-0.200 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac53e840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30452510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "59  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70]\n",
      "train_accuracy 0.974+/-0.054 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988]\n",
      "val_accuracy 0.825+/-0.125 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9 ]\n",
      "val_auc 0.886+/-0.117 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909]\n",
      "val_sens 0.805+/-0.146 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889]\n",
      "val_spec 0.853+/-0.115 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909]\n",
      "val_prec 0.843+/-0.128 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5]\n",
      "train_accuracy 0.815+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65 ]\n",
      "val_accuracy 0.624+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45]\n",
      "val_auc 0.674+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495]\n",
      "val_sens 0.588+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.   ]\n",
      "val_spec 0.651+/-0.122 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5  ]\n",
      "val_prec 0.569+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.   ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70768f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac0762f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "60  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274]\n",
      "train_accuracy 0.975+/-0.053 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.   ]\n",
      "val_accuracy 0.825+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85]\n",
      "val_auc 0.888+/-0.116 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98 ]\n",
      "val_sens 0.806+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875]\n",
      "val_spec 0.853+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833]\n",
      "val_prec 0.841+/-0.127 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34]\n",
      "train_accuracy 0.818+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95 ]\n",
      "val_accuracy 0.624+/-0.113 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65]\n",
      "val_auc 0.676+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758]\n",
      "val_sens 0.588+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583]\n",
      "val_spec 0.653+/-0.122 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75 ]\n",
      "val_prec 0.572+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee039f730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e70448510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80]\n",
      "train_accuracy 0.975+/-0.053 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.   ]\n",
      "val_accuracy 0.825+/-0.123 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85]\n",
      "val_auc 0.889+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92 ]\n",
      "val_sens 0.806+/-0.144 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818]\n",
      "val_spec 0.854+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889]\n",
      "val_prec 0.842+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12]\n",
      "train_accuracy 0.818+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825]\n",
      "val_accuracy 0.625+/-0.112 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65]\n",
      "val_auc 0.675+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61 ]\n",
      "val_sens 0.589+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667]\n",
      "val_spec 0.652+/-0.121 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636]\n",
      "val_prec 0.573+/-0.210 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed82b3048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30692510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "62  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166]\n",
      "train_accuracy 0.976+/-0.052 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.   ]\n",
      "val_accuracy 0.825+/-0.122 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8 ]\n",
      "val_auc 0.889+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94 ]\n",
      "val_sens 0.806+/-0.143 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8  ]\n",
      "val_spec 0.853+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8  ]\n",
      "val_prec 0.842+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22]\n",
      "train_accuracy 0.820+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962]\n",
      "val_accuracy 0.626+/-0.112 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7 ]\n",
      "val_auc 0.676+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76 ]\n",
      "val_sens 0.592+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75 ]\n",
      "val_spec 0.653+/-0.120 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667]\n",
      "val_prec 0.573+/-0.209 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3011e8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30695c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "63  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21]\n",
      "train_accuracy 0.975+/-0.053 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913]\n",
      "val_accuracy 0.821+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6 ]\n",
      "val_auc 0.886+/-0.117 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687]\n",
      "val_sens 0.802+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545]\n",
      "val_spec 0.850+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667]\n",
      "val_prec 0.839+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2]\n",
      "train_accuracy 0.815+/-0.171 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5  ]\n",
      "val_accuracy 0.621+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35]\n",
      "val_auc 0.672+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394]\n",
      "val_sens 0.588+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333]\n",
      "val_spec 0.648+/-0.124 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375]\n",
      "val_prec 0.571+/-0.208 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e706237b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac5196a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116]\n",
      "train_accuracy 0.975+/-0.052 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.   ]\n",
      "val_accuracy 0.821+/-0.123 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8 ]\n",
      "val_auc 0.886+/-0.116 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859]\n",
      "val_sens 0.802+/-0.144 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778]\n",
      "val_spec 0.849+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818]\n",
      "val_prec 0.838+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3]\n",
      "train_accuracy 0.811+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575]\n",
      "val_accuracy 0.620+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5 ]\n",
      "val_auc 0.669+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535]\n",
      "val_sens 0.585+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4  ]\n",
      "val_spec 0.646+/-0.124 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533]\n",
      "val_prec 0.566+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac69d6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e305a3510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "65  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000]\n",
      "train_accuracy 0.975+/-0.052 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.   ]\n",
      "val_accuracy 0.824+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.  ]\n",
      "val_auc 0.887+/-0.116 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.   ]\n",
      "val_sens 0.805+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.   ]\n",
      "val_spec 0.852+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.   ]\n",
      "val_prec 0.841+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60]\n",
      "train_accuracy 0.814+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988]\n",
      "val_accuracy 0.622+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8 ]\n",
      "val_auc 0.672+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838]\n",
      "val_sens 0.586+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692]\n",
      "val_spec 0.652+/-0.130 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.   ]\n",
      "val_prec 0.572+/-0.216 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.   ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac675158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30208d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "66  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78]\n",
      "train_accuracy 0.976+/-0.052 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.   ]\n",
      "val_accuracy 0.824+/-0.123 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85]\n",
      "val_auc 0.888+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93 ]\n",
      "val_sens 0.805+/-0.144 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818]\n",
      "val_spec 0.852+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889]\n",
      "val_prec 0.841+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4]\n",
      "train_accuracy 0.812+/-0.171 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688]\n",
      "val_accuracy 0.623+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7 ]\n",
      "val_auc 0.672+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68 ]\n",
      "val_sens 0.589+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75 ]\n",
      "val_spec 0.652+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667]\n",
      "val_prec 0.573+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ef138e620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94242f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "67  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000]\n",
      "train_accuracy 0.976+/-0.051 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.   ]\n",
      "val_accuracy 0.827+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.  ]\n",
      "val_auc 0.890+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.   ]\n",
      "val_sens 0.808+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.   ]\n",
      "val_spec 0.854+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.   ]\n",
      "val_prec 0.844+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19]\n",
      "train_accuracy 0.815+/-0.171 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988]\n",
      "val_accuracy 0.625+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7 ]\n",
      "val_auc 0.674+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77 ]\n",
      "val_sens 0.591+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75 ]\n",
      "val_spec 0.652+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667]\n",
      "val_prec 0.573+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5027f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30208b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "68  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23]\n",
      "train_accuracy 0.976+/-0.051 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95 ]\n",
      "val_accuracy 0.824+/-0.125 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65]\n",
      "val_auc 0.886+/-0.117 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667]\n",
      "val_sens 0.805+/-0.146 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583]\n",
      "val_spec 0.853+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75 ]\n",
      "val_prec 0.843+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2]\n",
      "train_accuracy 0.809+/-0.176 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438]\n",
      "val_accuracy 0.625+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65]\n",
      "val_auc 0.672+/-0.158 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596]\n",
      "val_sens 0.591+/-0.171 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583]\n",
      "val_spec 0.654+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75 ]\n",
      "val_prec 0.576+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3027e158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e942d3b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "69  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77]\n",
      "train_accuracy 0.976+/-0.051 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988]\n",
      "val_accuracy 0.825+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9 ]\n",
      "val_auc 0.887+/-0.116 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909]\n",
      "val_sens 0.805+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818]\n",
      "val_spec 0.855+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.   ]\n",
      "val_prec 0.845+/-0.126 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8]\n",
      "train_accuracy 0.809+/-0.175 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8  ]\n",
      "val_accuracy 0.623+/-0.116 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5 ]\n",
      "val_auc 0.672+/-0.157 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626]\n",
      "val_sens 0.589+/-0.171 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444]\n",
      "val_spec 0.652+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545]\n",
      "val_prec 0.574+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee060b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0273510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "70  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199]\n",
      "train_accuracy 0.976+/-0.050 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.   ]\n",
      "val_accuracy 0.826+/-0.124 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9 ]\n",
      "val_auc 0.888+/-0.116 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97 ]\n",
      "val_sens 0.806+/-0.145 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889]\n",
      "val_spec 0.856+/-0.114 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909]\n",
      "val_prec 0.846+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41]\n",
      "train_accuracy 0.812+/-0.175 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.   ]\n",
      "val_accuracy 0.624+/-0.115 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65]\n",
      "val_auc 0.673+/-0.156 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758]\n",
      "val_sens 0.589+/-0.170 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583]\n",
      "val_spec 0.653+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75 ]\n",
      "val_prec 0.577+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee039f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9452a0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69]\n",
      "train_accuracy 0.977+/-0.050 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.   ]\n",
      "val_accuracy 0.827+/-0.123 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9 ]\n",
      "val_auc 0.889+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93 ]\n",
      "val_sens 0.807+/-0.144 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9  ]\n",
      "val_spec 0.856+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9  ]\n",
      "val_prec 0.846+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7]\n",
      "train_accuracy 0.810+/-0.174 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7  ]\n",
      "val_accuracy 0.623+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6 ]\n",
      "val_auc 0.673+/-0.155 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7  ]\n",
      "val_sens 0.590+/-0.169 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667]\n",
      "val_spec 0.652+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571]\n",
      "val_prec 0.575+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed8041f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0236488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "72  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92]\n",
      "train_accuracy 0.977+/-0.050 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988]\n",
      "val_accuracy 0.826+/-0.123 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75]\n",
      "val_auc 0.887+/-0.115 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8  ]\n",
      "val_sens 0.806+/-0.143 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727]\n",
      "val_spec 0.855+/-0.113 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778]\n",
      "val_prec 0.846+/-0.123 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60]\n",
      "train_accuracy 0.813+/-0.175 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.   ]\n",
      "val_accuracy 0.624+/-0.114 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7 ]\n",
      "val_auc 0.677+/-0.156 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9  ]\n",
      "val_sens 0.592+/-0.169 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75 ]\n",
      "val_spec 0.653+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667]\n",
      "val_prec 0.575+/-0.209 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704ae400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e305156a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "73  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8]\n",
      "train_accuracy 0.971+/-0.068 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575]\n",
      "val_accuracy 0.821+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45]\n",
      "val_auc 0.880+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384]\n",
      "val_sens 0.801+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429]\n",
      "val_spec 0.850+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5  ]\n",
      "val_prec 0.843+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1]\n",
      "train_accuracy 0.810+/-0.175 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62 ]\n",
      "val_accuracy 0.620+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3 ]\n",
      "val_auc 0.671+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253]\n",
      "val_sens 0.586+/-0.176 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143]\n",
      "val_spec 0.649+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385]\n",
      "val_prec 0.569+/-0.215 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94532488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e305a0510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "74  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97]\n",
      "train_accuracy 0.971+/-0.068 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988]\n",
      "val_accuracy 0.822+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9 ]\n",
      "val_auc 0.881+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899]\n",
      "val_sens 0.801+/-0.148 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818]\n",
      "val_spec 0.852+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.   ]\n",
      "val_prec 0.845+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9]\n",
      "train_accuracy 0.811+/-0.174 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85 ]\n",
      "val_accuracy 0.620+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6 ]\n",
      "val_auc 0.671+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677]\n",
      "val_sens 0.587+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667]\n",
      "val_spec 0.648+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588]\n",
      "val_prec 0.564+/-0.217 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed812e400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50666d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126]\n",
      "train_accuracy 0.972+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.   ]\n",
      "val_accuracy 0.824+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95]\n",
      "val_auc 0.882+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99 ]\n",
      "val_sens 0.804+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.   ]\n",
      "val_spec 0.853+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917]\n",
      "val_prec 0.846+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29]\n",
      "train_accuracy 0.812+/-0.173 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938]\n",
      "val_accuracy 0.620+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65]\n",
      "val_auc 0.673+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808]\n",
      "val_sens 0.587+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6  ]\n",
      "val_spec 0.649+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7  ]\n",
      "val_prec 0.565+/-0.216 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e9403b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac4b38c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "76  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55]\n",
      "train_accuracy 0.971+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938]\n",
      "val_accuracy 0.823+/-0.128 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75]\n",
      "val_auc 0.881+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81 ]\n",
      "val_sens 0.804+/-0.148 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778]\n",
      "val_spec 0.852+/-0.119 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727]\n",
      "val_prec 0.844+/-0.124 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9]\n",
      "train_accuracy 0.811+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712]\n",
      "val_accuracy 0.618+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45]\n",
      "val_auc 0.672+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66 ]\n",
      "val_sens 0.585+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444]\n",
      "val_spec 0.646+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455]\n",
      "val_prec 0.563+/-0.215 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0273950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0273c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "77  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000]\n",
      "train_accuracy 0.972+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.   ]\n",
      "val_accuracy 0.825+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.  ]\n",
      "val_auc 0.883+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.   ]\n",
      "val_sens 0.806+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.   ]\n",
      "val_spec 0.854+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.   ]\n",
      "val_prec 0.846+/-0.125 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21]\n",
      "train_accuracy 0.813+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962]\n",
      "val_accuracy 0.619+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7 ]\n",
      "val_auc 0.673+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74 ]\n",
      "val_sens 0.588+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75 ]\n",
      "val_spec 0.646+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667]\n",
      "val_prec 0.564+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6  ]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee06ef730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e304206a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "78  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16]\n",
      "train_accuracy 0.969+/-0.069 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788]\n",
      "val_accuracy 0.821+/-0.135 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45]\n",
      "val_auc 0.879+/-0.130 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576]\n",
      "val_sens 0.801+/-0.155 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375]\n",
      "val_spec 0.849+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5  ]\n",
      "val_prec 0.840+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1]\n",
      "train_accuracy 0.810+/-0.172 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61 ]\n",
      "val_accuracy 0.616+/-0.120 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4 ]\n",
      "val_auc 0.669+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323]\n",
      "val_sens 0.584+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333]\n",
      "val_spec 0.644+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455]\n",
      "val_prec 0.561+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac675620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e947aa7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "79  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59]\n",
      "train_accuracy 0.969+/-0.069 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962]\n",
      "val_accuracy 0.820+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75]\n",
      "val_auc 0.879+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879]\n",
      "val_sens 0.800+/-0.155 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75 ]\n",
      "val_spec 0.848+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75 ]\n",
      "val_prec 0.837+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9]\n",
      "train_accuracy 0.810+/-0.171 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775]\n",
      "val_accuracy 0.618+/-0.120 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75]\n",
      "val_auc 0.671+/-0.164 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848]\n",
      "val_sens 0.587+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833]\n",
      "val_spec 0.645+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714]\n",
      "val_prec 0.561+/-0.213 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556]\n",
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac696a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac192400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "80  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214]\n",
      "train_accuracy 0.970+/-0.069 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.   ]\n",
      "val_accuracy 0.821+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95]\n",
      "val_auc 0.880+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99 ]\n",
      "val_sens 0.801+/-0.154 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9  ]\n",
      "val_spec 0.850+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.   ]\n",
      "val_prec 0.839+/-0.138 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13]\n",
      "train_accuracy 0.811+/-0.170 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875]\n",
      "val_accuracy 0.618+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65]\n",
      "val_auc 0.670+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606]\n",
      "val_sens 0.587+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583]\n",
      "val_spec 0.646+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75 ]\n",
      "val_prec 0.563+/-0.213 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0555158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e945a8510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "81  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46]\n",
      "train_accuracy 0.970+/-0.068 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.   ]\n",
      "val_accuracy 0.822+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85]\n",
      "val_auc 0.881+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96 ]\n",
      "val_sens 0.801+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818]\n",
      "val_spec 0.850+/-0.125 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889]\n",
      "val_prec 0.840+/-0.137 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15]\n",
      "train_accuracy 0.811+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825]\n",
      "val_accuracy 0.619+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65]\n",
      "val_auc 0.671+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73 ]\n",
      "val_sens 0.589+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714]\n",
      "val_spec 0.646+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615]\n",
      "val_prec 0.563+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac54f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee040b510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "82  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359]\n",
      "train_accuracy 0.970+/-0.068 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.   ]\n",
      "val_accuracy 0.823+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9 ]\n",
      "val_auc 0.882+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94 ]\n",
      "val_sens 0.803+/-0.152 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9  ]\n",
      "val_spec 0.851+/-0.124 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9  ]\n",
      "val_prec 0.841+/-0.136 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26]\n",
      "train_accuracy 0.812+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925]\n",
      "val_accuracy 0.619+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65]\n",
      "val_auc 0.673+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82 ]\n",
      "val_sens 0.590+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636]\n",
      "val_spec 0.646+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667]\n",
      "val_prec 0.564+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5009c598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5009c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "83  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33]\n",
      "train_accuracy 0.970+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975]\n",
      "val_accuracy 0.821+/-0.133 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7 ]\n",
      "val_auc 0.879+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667]\n",
      "val_sens 0.801+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636]\n",
      "val_spec 0.850+/-0.123 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778]\n",
      "val_prec 0.840+/-0.136 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4]\n",
      "train_accuracy 0.810+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6  ]\n",
      "val_accuracy 0.617+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45]\n",
      "val_auc 0.670+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465]\n",
      "val_sens 0.588+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429]\n",
      "val_spec 0.644+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5  ]\n",
      "val_prec 0.566+/-0.210 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac6b80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e50696378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "84  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95]\n",
      "train_accuracy 0.971+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988]\n",
      "val_accuracy 0.822+/-0.132 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9 ]\n",
      "val_auc 0.880+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939]\n",
      "val_sens 0.802+/-0.152 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889]\n",
      "val_spec 0.851+/-0.123 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909]\n",
      "val_prec 0.841+/-0.135 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25]\n",
      "train_accuracy 0.811+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95 ]\n",
      "val_accuracy 0.619+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8 ]\n",
      "val_auc 0.673+/-0.163 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859]\n",
      "val_sens 0.591+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857]\n",
      "val_spec 0.646+/-0.127 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769]\n",
      "val_prec 0.567+/-0.209 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3058fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e704c1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "85  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426]\n",
      "train_accuracy 0.971+/-0.067 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.   ]\n",
      "val_accuracy 0.823+/-0.132 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9 ]\n",
      "val_auc 0.881+/-0.129 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98 ]\n",
      "val_sens 0.803+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889]\n",
      "val_spec 0.851+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909]\n",
      "val_prec 0.841+/-0.134 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26]\n",
      "train_accuracy 0.812+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9  ]\n",
      "val_accuracy 0.619+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6 ]\n",
      "val_auc 0.673+/-0.162 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667]\n",
      "val_sens 0.590+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556]\n",
      "val_spec 0.646+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636]\n",
      "val_prec 0.567+/-0.208 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e941521e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5059b7b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "86  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52]\n",
      "train_accuracy 0.971+/-0.066 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962]\n",
      "val_accuracy 0.823+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85]\n",
      "val_auc 0.881+/-0.128 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87 ]\n",
      "val_sens 0.803+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818]\n",
      "val_spec 0.852+/-0.122 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889]\n",
      "val_prec 0.842+/-0.134 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11]\n",
      "train_accuracy 0.813+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9  ]\n",
      "val_accuracy 0.620+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75]\n",
      "val_auc 0.674+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76 ]\n",
      "val_sens 0.593+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778]\n",
      "val_spec 0.647+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727]\n",
      "val_prec 0.568+/-0.207 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e944fd400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e7030b950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "87  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60]\n",
      "train_accuracy 0.971+/-0.066 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.   ]\n",
      "val_accuracy 0.824+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85]\n",
      "val_auc 0.881+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91 ]\n",
      "val_sens 0.803+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818]\n",
      "val_spec 0.852+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889]\n",
      "val_prec 0.843+/-0.133 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33]\n",
      "train_accuracy 0.815+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95 ]\n",
      "val_accuracy 0.621+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7 ]\n",
      "val_auc 0.675+/-0.161 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8  ]\n",
      "val_sens 0.593+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667]\n",
      "val_spec 0.648+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75 ]\n",
      "val_prec 0.571+/-0.207 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e30447620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac7c6510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "88  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39]\n",
      "train_accuracy 0.971+/-0.066 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962]\n",
      "val_accuracy 0.822+/-0.131 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65]\n",
      "val_auc 0.880+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768]\n",
      "val_sens 0.801+/-0.151 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583]\n",
      "val_spec 0.851+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75 ]\n",
      "val_prec 0.842+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2]\n",
      "train_accuracy 0.812+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587]\n",
      "val_accuracy 0.621+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6 ]\n",
      "val_auc 0.676+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788]\n",
      "val_sens 0.598+/-0.177 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.   ]\n",
      "val_spec 0.647+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579]\n",
      "val_prec 0.566+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac69d620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e305a0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "89  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50]\n",
      "train_accuracy 0.971+/-0.065 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975]\n",
      "val_accuracy 0.821+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75]\n",
      "val_auc 0.880+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879]\n",
      "val_sens 0.799+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7  ]\n",
      "val_spec 0.850+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8  ]\n",
      "val_prec 0.841+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20]\n",
      "train_accuracy 0.814+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925]\n",
      "val_accuracy 0.621+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6 ]\n",
      "val_auc 0.676+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697]\n",
      "val_sens 0.598+/-0.176 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6  ]\n",
      "val_spec 0.647+/-0.124 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6  ]\n",
      "val_prec 0.563+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac373ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed804c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "90  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119]\n",
      "train_accuracy 0.972+/-0.065 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.   ]\n",
      "val_accuracy 0.822+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95]\n",
      "val_auc 0.881+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97 ]\n",
      "val_sens 0.801+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9  ]\n",
      "val_spec 0.852+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.   ]\n",
      "val_prec 0.843+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35]\n",
      "train_accuracy 0.816+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988]\n",
      "val_accuracy 0.623+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85]\n",
      "val_auc 0.678+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838]\n",
      "val_sens 0.600+/-0.176 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8  ]\n",
      "val_spec 0.649+/-0.126 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9  ]\n",
      "val_prec 0.567+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac434730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0408730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "91  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51]\n",
      "train_accuracy 0.972+/-0.065 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975]\n",
      "val_accuracy 0.823+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85]\n",
      "val_auc 0.881+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89 ]\n",
      "val_sens 0.801+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818]\n",
      "val_spec 0.853+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889]\n",
      "val_prec 0.844+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16]\n",
      "train_accuracy 0.816+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85 ]\n",
      "val_accuracy 0.624+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7 ]\n",
      "val_auc 0.678+/-0.158 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65 ]\n",
      "val_sens 0.602+/-0.176 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75 ]\n",
      "val_spec 0.650+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667]\n",
      "val_prec 0.567+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e507e2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e703fa510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "92  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000]\n",
      "train_accuracy 0.972+/-0.064 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.   ]\n",
      "val_accuracy 0.824+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.  ]\n",
      "val_auc 0.882+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.   ]\n",
      "val_sens 0.803+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.   ]\n",
      "val_spec 0.854+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.   ]\n",
      "val_prec 0.845+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22]\n",
      "train_accuracy 0.817+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95 ]\n",
      "val_accuracy 0.624+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65]\n",
      "val_auc 0.678+/-0.158 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7  ]\n",
      "val_sens 0.602+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636]\n",
      "val_spec 0.650+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667]\n",
      "val_prec 0.568+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed805ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e3052b488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "93  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20]\n",
      "train_accuracy 0.972+/-0.064 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962]\n",
      "val_accuracy 0.823+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7 ]\n",
      "val_auc 0.882+/-0.125 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818]\n",
      "val_sens 0.801+/-0.150 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636]\n",
      "val_spec 0.853+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778]\n",
      "val_prec 0.845+/-0.131 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1]\n",
      "train_accuracy 0.815+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54 ]\n",
      "val_accuracy 0.623+/-0.119 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45]\n",
      "val_auc 0.675+/-0.160 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384]\n",
      "val_sens 0.600+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375]\n",
      "val_spec 0.648+/-0.125 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5  ]\n",
      "val_prec 0.566+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee2865d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e7025fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "94  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41]\n",
      "train_accuracy 0.972+/-0.064 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962]\n",
      "val_accuracy 0.823+/-0.130 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85]\n",
      "val_auc 0.882+/-0.124 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869]\n",
      "val_sens 0.801+/-0.149 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75 ]\n",
      "val_spec 0.855+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.   ]\n",
      "val_prec 0.846+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18]\n",
      "train_accuracy 0.816+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95 ]\n",
      "val_accuracy 0.622+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6 ]\n",
      "val_auc 0.675+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677]\n",
      "val_sens 0.599+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545]\n",
      "val_spec 0.648+/-0.124 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667]\n",
      "val_prec 0.567+/-0.211 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ed811a730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e5005f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "95  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131]\n",
      "train_accuracy 0.972+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.   ]\n",
      "val_accuracy 0.824+/-0.129 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9 ]\n",
      "val_auc 0.882+/-0.124 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96 ]\n",
      "val_sens 0.801+/-0.148 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818]\n",
      "val_spec 0.856+/-0.121 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.   ]\n",
      "val_prec 0.848+/-0.132 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30]\n",
      "train_accuracy 0.817+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913]\n",
      "val_accuracy 0.624+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75]\n",
      "val_auc 0.677+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818]\n",
      "val_sens 0.600+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643]\n",
      "val_spec 0.652+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.   ]\n",
      "val_prec 0.572+/-0.215 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.   ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e307ba510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac373378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "96  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65]\n",
      "train_accuracy 0.972+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.   ]\n",
      "val_accuracy 0.824+/-0.128 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8 ]\n",
      "val_auc 0.883+/-0.123 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9  ]\n",
      "val_sens 0.801+/-0.147 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8  ]\n",
      "val_spec 0.856+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8  ]\n",
      "val_prec 0.847+/-0.131 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11]\n",
      "train_accuracy 0.817+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837]\n",
      "val_accuracy 0.624+/-0.118 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65]\n",
      "val_auc 0.676+/-0.158 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61 ]\n",
      "val_sens 0.601+/-0.173 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714]\n",
      "val_spec 0.652+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615]\n",
      "val_prec 0.571+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e94408ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e307d9d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "97  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65, 179]\n",
      "train_accuracy 0.973+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.\n",
      " 1.   ]\n",
      "val_accuracy 0.824+/-0.128 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8  0.8 ]\n",
      "val_auc 0.883+/-0.123 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9\n",
      " 0.95 ]\n",
      "val_sens 0.802+/-0.147 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8\n",
      " 0.875]\n",
      "val_spec 0.855+/-0.120 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8\n",
      " 0.75 ]\n",
      "val_prec 0.846+/-0.131 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8\n",
      " 0.7  ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11, 20]\n",
      "train_accuracy 0.819+/-0.166 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837\n",
      " 0.95 ]\n",
      "val_accuracy 0.623+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65 0.55]\n",
      "val_auc 0.676+/-0.157 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61\n",
      " 0.7  ]\n",
      "val_sens 0.601+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714\n",
      " 0.571]\n",
      "val_spec 0.650+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615\n",
      " 0.538]\n",
      "val_prec 0.569+/-0.213 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5\n",
      " 0.4  ]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e302b40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0273c80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "98  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65, 179, 28]\n",
      "train_accuracy 0.972+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.\n",
      " 1.    0.9  ]\n",
      "val_accuracy 0.819+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8  0.8  0.4 ]\n",
      "val_auc 0.880+/-0.127 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9\n",
      " 0.95  0.545]\n",
      "val_sens 0.797+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8\n",
      " 0.875 0.333]\n",
      "val_spec 0.851+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8\n",
      " 0.75  0.455]\n",
      "val_prec 0.841+/-0.140 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8\n",
      " 0.7   0.333]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11, 20, 3]\n",
      "train_accuracy 0.816+/-0.167 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837\n",
      " 0.95  0.55 ]\n",
      "val_accuracy 0.622+/-0.117 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65 0.55 0.5 ]\n",
      "val_auc 0.675+/-0.157 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61\n",
      " 0.7   0.576]\n",
      "val_sens 0.599+/-0.172 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714\n",
      " 0.571 0.444]\n",
      "val_spec 0.649+/-0.128 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615\n",
      " 0.538 0.545]\n",
      "val_prec 0.568+/-0.212 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5\n",
      " 0.4   0.444]\n",
      "vanilla model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ee0632378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac434e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "99  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65, 179, 28, 68]\n",
      "train_accuracy 0.972+/-0.063 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.\n",
      " 1.    0.9   0.988]\n",
      "val_accuracy 0.820+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8  0.8  0.4\n",
      " 0.9 ]\n",
      "val_auc 0.881+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9\n",
      " 0.95  0.545 0.96 ]\n",
      "val_sens 0.797+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8\n",
      " 0.875 0.333 0.818]\n",
      "val_spec 0.852+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8\n",
      " 0.75  0.455 1.   ]\n",
      "val_prec 0.842+/-0.141 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8\n",
      " 0.7   0.333 1.   ]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11, 20, 3, 3]\n",
      "train_accuracy 0.813+/-0.169 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837\n",
      " 0.95  0.55  0.55 ]\n",
      "val_accuracy 0.619+/-0.120 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65 0.55 0.5\n",
      " 0.35]\n",
      "val_auc 0.672+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61\n",
      " 0.7   0.576 0.374]\n",
      "val_sens 0.596+/-0.175 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714\n",
      " 0.571 0.444 0.25 ]\n",
      "val_spec 0.647+/-0.129 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615\n",
      " 0.538 0.545 0.417]\n",
      "val_prec 0.564+/-0.214 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5\n",
      " 0.4   0.444 0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model training...\n",
      "vanilla model trained\n",
      "gapnet 1st model training...\n",
      "gapnet 1st model trained\n",
      "gapnet 2nd model training...\n",
      "gapnet 2nd model trained\n",
      "gapnet full model training...\n",
      "gapnet full model trained\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8eac5b5730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8e942f8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "100  fold(s) finished\n",
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65, 179, 28, 68, 855]\n",
      "train_accuracy 0.972+/-0.062 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.\n",
      " 1.    0.9   0.988 1.   ]\n",
      "val_accuracy 0.822+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8  0.8  0.4\n",
      " 0.9  0.95]\n",
      "val_auc 0.882+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9\n",
      " 0.95  0.545 0.96  1.   ]\n",
      "val_sens 0.799+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8\n",
      " 0.875 0.333 0.818 1.   ]\n",
      "val_spec 0.853+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8\n",
      " 0.75  0.455 1.    0.917]\n",
      "val_prec 0.843+/-0.140 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8\n",
      " 0.7   0.333 1.    0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11, 20, 3, 3, 17]\n",
      "train_accuracy 0.813+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837\n",
      " 0.95  0.55  0.55  0.85 ]\n",
      "val_accuracy 0.621+/-0.121 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65 0.55 0.5\n",
      " 0.35 0.8 ]\n",
      "val_auc 0.674+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61\n",
      " 0.7   0.576 0.374 0.818]\n",
      "val_sens 0.597+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714\n",
      " 0.571 0.444 0.25  0.727]\n",
      "val_spec 0.649+/-0.131 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615\n",
      " 0.538 0.545 0.417 0.889]\n",
      "val_prec 0.568+/-0.216 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5\n",
      " 0.4   0.444 0.222 0.889]\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 256\n",
    "gapnet_best_epochs = []\n",
    "gapnet_train_accuracies, gapnet_val_accuracies, gapnet_test_accuracies = [], [], []\n",
    "gapnet_val_aucs, gapnet_val_precisions, gapnet_val_sensitivities, gapnet_val_specificities = [], [], [], []\n",
    "gapnet_val_y_preds, gapnet_val_y_labels = [], []\n",
    "\n",
    "vanilla_best_epochs = []\n",
    "vanilla_train_accuracies, vanilla_val_accuracies, vanilla_test_accuracies = [], [], []\n",
    "vanilla_val_aucs, vanilla_val_precisions, vanilla_val_sensitivities, vanilla_val_specificities = [], [], [], []\n",
    "vanilla_val_y_preds, vanilla_val_y_labels = [], []\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=0,\n",
    "    patience=30,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "runs = 100 #100 95 90 etc\n",
    "num = int(runs / 5)\n",
    "\n",
    "for i in range(num):\n",
    "    for train_index, test_index in skf.split(X_overlap, y_overlap):\n",
    "        # train/test split\n",
    "        X_train, X_test = X_overlap[train_index], X_overlap[test_index]\n",
    "        y_train, y_test = y_overlap[train_index], y_overlap[test_index]\n",
    "    \n",
    "        # wrap up the incomplete dataset\n",
    "        X_modality_1 = np.concatenate((X_train[:,0:feature_num_modality_1], X[-mismatch:,0:feature_num_modality_1]), axis=0)\n",
    "        Y_modality_1 = np.concatenate((y_train, y[-mismatch:]), axis=0)\n",
    "        X_modality_2 = np.concatenate((X_train[:,-feature_num_modality_2:], X[0:mismatch,-feature_num_modality_2:]), axis=0)\n",
    "        Y_modality_2 = np.concatenate((y_train, y[0:mismatch]), axis=0)\n",
    "    \n",
    "        # split train dataset\n",
    "        X_train_for_modality_1 = X_train[:,0:feature_num_modality_1]\n",
    "        X_train_for_modality_2 = X_train[:,-feature_num_modality_2:]\n",
    "        X_test_for_modality_1 = X_test[:,0:feature_num_modality_1]\n",
    "        X_test_for_modality_2 = X_test[:,-feature_num_modality_2:]\n",
    "    \n",
    "        # normalization\n",
    "        thres = 20\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        X_train = np.clip(X_train, -thres, thres)\n",
    "        X_test = np.clip(X_test, -thres, thres)\n",
    "    \n",
    "        X_modality_1 = scaler.fit_transform(X_modality_1)\n",
    "        X_train_for_modality_1 = scaler.transform(X_train_for_modality_1)\n",
    "        X_test_for_modality_1 = scaler.transform(X_test_for_modality_1)\n",
    "    \n",
    "        X_modality_1 = np.clip(X_modality_1, -thres, thres)\n",
    "        X_train_for_modality_1 = np.clip(X_train_for_modality_1, -thres, thres)\n",
    "        X_test_for_modality_1 = np.clip(X_test_for_modality_1, -thres, thres)\n",
    "    \n",
    "        X__modality_2 = scaler.fit_transform(X_modality_2)\n",
    "        X_train_for_modality_2 = scaler.transform(X_train_for_modality_2)\n",
    "        X_test_for_modality_2 = scaler.transform(X_test_for_modality_2)\n",
    "    \n",
    "        X_modality_2 = np.clip(X_modality_2, -thres, thres)\n",
    "        X_train_for_modality_2 = np.clip(X_train_for_modality_2, -thres, thres)\n",
    "        X_test_for_modality_2 = np.clip(X_test_for_modality_2, -thres, thres)\n",
    "    \n",
    "        # vanilla model building\n",
    "        input_0 = tf.keras.layers.Input(shape=(feature_num,))\n",
    "        hidden_0 = tf.keras.layers.Dense(2*feature_num, activation=\"relu\")\n",
    "        hidden_1 = tf.keras.layers.Dense(2*feature_num, activation=\"relu\")\n",
    "        outputs_0_pretrained = (hidden_0)(input_0)\n",
    "        outputs_0_pretrained = (hidden_1)(outputs_0_pretrained)\n",
    "        outputs_0 = tf.keras.layers.Dropout(0.5)(outputs_0_pretrained)\n",
    "        outputs_0 = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(outputs_0)\n",
    "        model_0 = tf.keras.Model(inputs=input_0, outputs=[outputs_0])\n",
    "        model_0.compile(optimizer=\"adam\", loss=tf.keras.losses.categorical_crossentropy, metrics=METRICS)\n",
    "    \n",
    "        print('vanilla model training...')\n",
    "        vanilla_history = model_0.fit(X_train, to_categorical(y_train), epochs=EPOCHS, verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=(X_test, to_categorical(y_test)))\n",
    "        print('vanilla model trained')\n",
    "    \n",
    "        # GapNet model building\n",
    "        ## model 1\n",
    "        input_1 = tf.keras.layers.Input(shape=(feature_num_modality_1,))\n",
    "        hidden_1 = tf.keras.layers.Dense(2*feature_num_modality_1, activation=\"relu\")\n",
    "        hidden_1_1 = tf.keras.layers.Dense(2*feature_num, activation=\"relu\")\n",
    "        outputs_1_pretrained = (hidden_1)(input_1)\n",
    "        outputs_1_pretrained = (hidden_1_1)(outputs_1_pretrained)\n",
    "        outputs_1 = tf.keras.layers.Dropout(0.5)(outputs_1_pretrained)\n",
    "        outputs_1 = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(outputs_1)\n",
    "        model_1 = tf.keras.Model(inputs=input_1, outputs=[outputs_1])\n",
    "        model_1.compile(optimizer=\"adam\", loss=tf.keras.losses.BinaryCrossentropy(), metrics=METRICS)\n",
    "        print('gapnet 1st model training...')\n",
    "        model_1.fit(X_modality_1, to_categorical(Y_modality_1), epochs=EPOCHS, verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=(X_test_for_modality_1, to_categorical(y_test)))\n",
    "        print('gapnet 1st model trained')\n",
    "    \n",
    "        ## model 2\n",
    "        input_2 = tf.keras.layers.Input(shape=(feature_num_modality_2,))\n",
    "        hidden_2 = tf.keras.layers.Dense(2*feature_num_modality_2, activation=\"relu\")\n",
    "        hidden_2_1 = tf.keras.layers.Dense(2*feature_num_modality_2, activation=\"relu\")\n",
    "        outputs_2_pretrained = (hidden_2)(input_2)\n",
    "        outputs_2_pretrained = (hidden_2_1)(outputs_2_pretrained)\n",
    "        outputs_2 = tf.keras.layers.Dropout(0.5)(outputs_2_pretrained)\n",
    "        outputs_2 = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(outputs_2)\n",
    "        model_2 = tf.keras.Model(inputs=input_2, outputs=[outputs_2])\n",
    "        model_2.compile(optimizer=\"adam\", loss=tf.keras.losses.BinaryCrossentropy(), metrics=METRICS)\n",
    "        print('gapnet 2nd model training...')\n",
    "        model_2.fit(X_modality_2, to_categorical(Y_modality_2), epochs=EPOCHS, verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=(X_test_for_modality_2, to_categorical(y_test)))\n",
    "        print('gapnet 2nd model trained')\n",
    "    \n",
    "        ## models concatenating\n",
    "        concat = tf.keras.layers.Concatenate()([outputs_1_pretrained, outputs_2_pretrained])\n",
    "        output_con = tf.keras.layers.Dropout(0.0)(concat)\n",
    "        output_con = tf.keras.layers.Dense(units=n_classes, activation=tf.keras.activations.softmax, name=\"output_con\")(output_con)\n",
    "        full_model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output_con])\n",
    "        hidden_2.trainable = False\n",
    "        hidden_1.trainable = False\n",
    "        full_model.compile(optimizer=\"adam\", loss=tf.keras.losses.BinaryCrossentropy(), metrics=METRICS)\n",
    "    \n",
    "        print('gapnet full model training...')\n",
    "        gapnet_history = full_model.fit(\n",
    "        [X_train_for_modality_1, X_train_for_modality_2],\n",
    "        to_categorical(y_train),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS, verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "        validation_data=([X_test_for_modality_1, X_test_for_modality_2], to_categorical(y_test)))\n",
    "        print('gapnet full model trained')\n",
    "    \n",
    "        # score recording\n",
    "        ## gapnet\n",
    "        best_epoch = np.argmin(gapnet_history.history['val_loss'])+1\n",
    "        train_accuracy = gapnet_history.history['accuracy'][best_epoch-1]\n",
    "        val_y_pred = full_model.predict([X_test_for_modality_1, X_test_for_modality_2])\n",
    "        threshold = 0.5\n",
    "        val_y_pred_class = np.where(val_y_pred>threshold, 1, 0)\n",
    "        m = tf.keras.metrics.TruePositives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_tp = m.result().numpy()\n",
    "        m = tf.keras.metrics.TrueNegatives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_tn = m.result().numpy()\n",
    "        m = tf.keras.metrics.FalsePositives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_fp = m.result().numpy()\n",
    "        m = tf.keras.metrics.FalseNegatives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_fn = m.result().numpy()\n",
    "        val_accuracy = (val_tp+val_tn)/(val_tp+val_tn+val_fp+val_fn)\n",
    "        val_sensitivity = (val_tp)/(val_tp+val_fn)\n",
    "        val_specificity = (val_tn)/(val_tn+val_fp)\n",
    "        val_precision = (val_tp)/(val_tp+val_fp)\n",
    "        val_auc = roc_auc_score(y_test, val_y_pred[:,1])\n",
    "        \n",
    "        \n",
    "        gapnet_best_epochs.append(best_epoch)\n",
    "        gapnet_train_accuracies.append(train_accuracy)\n",
    "        gapnet_val_accuracies.append(val_accuracy)\n",
    "        gapnet_val_aucs.append(val_auc)\n",
    "        gapnet_val_precisions.append(val_precision)\n",
    "        gapnet_val_sensitivities.append(val_sensitivity)\n",
    "        gapnet_val_specificities.append(val_specificity)\n",
    "        #gapnet_val_y_preds.append(val_y_pred[:,1])\n",
    "        gapnet_val_y_preds = np.append(gapnet_val_y_preds, val_y_pred[:,1])\n",
    "        #gapnet_val_y_labels.append(y_test)\n",
    "        gapnet_val_y_labels = np.append(gapnet_val_y_labels, y_test)\n",
    "    \n",
    "        ## vanilla\n",
    "        best_epoch = np.argmin(vanilla_history.history['val_loss'])+1\n",
    "        train_accuracy = vanilla_history.history['accuracy'][best_epoch-1]\n",
    "        val_auc = vanilla_history.history['val_auc'][best_epoch-1]\n",
    "        val_y_pred = model_0.predict(X_test)\n",
    "        threshold = 0.5\n",
    "        val_y_pred_class = np.where(val_y_pred>threshold, 1, 0)\n",
    "        m = tf.keras.metrics.TruePositives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_tp = m.result().numpy()\n",
    "        m = tf.keras.metrics.TrueNegatives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_tn = m.result().numpy()\n",
    "        m = tf.keras.metrics.FalsePositives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_fp = m.result().numpy()\n",
    "        m = tf.keras.metrics.FalseNegatives()\n",
    "        m.update_state(val_y_pred_class[:,1], y_test)\n",
    "        val_fn = m.result().numpy()\n",
    "        \n",
    "        val_accuracy = (val_tp+val_tn)/(val_tp+val_tn+val_fp+val_fn)\n",
    "        val_sensitivity = (val_tp)/(val_tp+val_fn)\n",
    "        val_specificity = (val_tn)/(val_tn+val_fp)\n",
    "        val_precision = (val_tp)/(val_tp+val_fp)\n",
    "        val_auc = roc_auc_score(y_test, val_y_pred[:,1])\n",
    "        \n",
    "        vanilla_best_epochs.append(best_epoch)\n",
    "        vanilla_train_accuracies.append(train_accuracy)\n",
    "        vanilla_val_accuracies.append(val_accuracy)\n",
    "        vanilla_val_aucs.append(val_auc)\n",
    "        vanilla_val_precisions.append(val_precision)\n",
    "        vanilla_val_sensitivities.append(val_sensitivity)\n",
    "        vanilla_val_specificities.append(val_specificity)\n",
    "        vanilla_val_y_preds = np.append(vanilla_val_y_preds,val_y_pred[:,1])\n",
    "        vanilla_val_y_labels = np.append(vanilla_val_y_labels, y_test)\n",
    "    \n",
    "        print(fold, ' fold(s) finished')\n",
    "        fold+=1\n",
    "        print(\"GapNet Results :\")\n",
    "        print(\"best_epochs {}\".format(gapnet_best_epochs))\n",
    "        print(\"train_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_train_accuracies), np.std(gapnet_train_accuracies), np.round(gapnet_train_accuracies, 3)))\n",
    "        print(\"val_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_accuracies), np.std(gapnet_val_accuracies), np.round(gapnet_val_accuracies, 3)))\n",
    "        print(\"val_auc {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_aucs), np.std(gapnet_val_aucs), np.round(gapnet_val_aucs, 3)))\n",
    "        print(\"val_sens {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_sensitivities), np.std(gapnet_val_sensitivities), np.round(gapnet_val_sensitivities, 3)))\n",
    "        print(\"val_spec {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_specificities), np.std(gapnet_val_specificities), np.round(gapnet_val_specificities, 3)))\n",
    "        print(\"val_prec {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_precisions), np.std(gapnet_val_precisions), np.round(gapnet_val_precisions, 3)))\n",
    "        print(\"Vanilla Results :\")\n",
    "        print(\"best_epochs {}\".format(vanilla_best_epochs))\n",
    "        print(\"train_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_train_accuracies), np.std(vanilla_train_accuracies), np.round(vanilla_train_accuracies, 3)))\n",
    "        print(\"val_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_accuracies), np.std(vanilla_val_accuracies), np.round(vanilla_val_accuracies, 3)))\n",
    "        print(\"val_auc {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_aucs), np.std(vanilla_val_aucs), np.round(vanilla_val_aucs, 3)))\n",
    "        print(\"val_sens {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_sensitivities), np.std(vanilla_val_sensitivities), np.round(vanilla_val_sensitivities, 3)))\n",
    "        print(\"val_spec {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_specificities), np.std(vanilla_val_specificities), np.round(vanilla_val_specificities, 3)))\n",
    "        print(\"val_prec {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_precisions), np.std(vanilla_val_precisions), np.round(vanilla_val_precisions, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GapNet Results :\n",
      "best_epochs [94, 161, 8, 65, 303, 136, 967, 35, 72, 600, 36, 361, 34, 47, 797, 62, 469, 26, 79, 423, 53, 100, 21, 67, 2000, 44, 184, 13, 42, 491, 34, 397, 25, 70, 164, 43, 477, 33, 53, 85, 84, 344, 29, 79, 2000, 72, 101, 32, 75, 495, 61, 182, 31, 58, 171, 81, 444, 15, 70, 274, 80, 166, 21, 116, 2000, 78, 2000, 23, 77, 199, 69, 92, 8, 97, 126, 55, 2000, 16, 59, 214, 46, 359, 33, 95, 426, 52, 60, 39, 50, 119, 51, 2000, 20, 41, 131, 65, 179, 28, 68, 855]\n",
      "train_accuracy 0.972+/-0.062 : [0.988 1.    0.625 0.988 1.    1.    1.    0.988 1.    1.    0.887 1.\n",
      " 0.962 0.962 1.    0.988 1.    0.938 1.    1.    0.925 1.    0.988 0.962\n",
      " 1.    0.962 1.    0.913 0.962 1.    0.938 1.    0.95  0.975 0.988 0.925\n",
      " 1.    0.95  0.988 0.988 1.    1.    0.925 0.988 1.    0.988 1.    0.975\n",
      " 1.    1.    1.    1.    0.962 1.    1.    1.    1.    0.925 0.988 1.\n",
      " 1.    1.    0.913 1.    1.    1.    1.    0.95  0.988 1.    1.    0.988\n",
      " 0.575 0.988 1.    0.938 1.    0.788 0.962 1.    1.    1.    0.975 0.988\n",
      " 1.    0.962 1.    0.962 0.975 1.    0.975 1.    0.962 0.962 1.    1.\n",
      " 1.    0.9   0.988 1.   ]\n",
      "val_accuracy 0.822+/-0.134 : [0.9  0.95 0.45 0.85 0.85 0.9  0.9  0.7  0.95 0.9  0.7  0.9  0.65 0.75\n",
      " 0.95 0.8  1.   0.65 0.8  0.9  0.75 0.85 0.6  0.8  1.   0.75 0.95 0.5\n",
      " 0.9  1.   0.75 0.9  0.6  0.8  0.85 0.8  1.   0.7  0.8  0.9  0.85 0.9\n",
      " 0.65 0.85 1.   0.8  0.85 0.75 0.85 0.95 0.8  0.95 0.6  0.8  0.9  0.9\n",
      " 0.95 0.75 0.9  0.85 0.85 0.8  0.6  0.8  1.   0.85 1.   0.65 0.9  0.9\n",
      " 0.9  0.75 0.45 0.9  0.95 0.75 1.   0.45 0.75 0.95 0.85 0.9  0.7  0.9\n",
      " 0.9  0.85 0.85 0.65 0.75 0.95 0.85 1.   0.7  0.85 0.9  0.8  0.8  0.4\n",
      " 0.9  0.95]\n",
      "val_auc 0.882+/-0.126 : [0.89  1.    0.455 0.919 0.98  0.96  1.    0.687 0.949 0.99  0.84  0.96\n",
      " 0.727 0.798 1.    0.85  1.    0.768 0.939 0.99  0.87  0.98  0.747 0.828\n",
      " 1.    0.87  0.94  0.636 0.879 1.    0.78  0.99  0.737 0.899 0.98  0.88\n",
      " 1.    0.758 0.879 0.949 0.96  0.96  0.596 0.96  1.    0.91  0.93  0.788\n",
      " 0.909 0.96  0.87  0.97  0.717 0.929 0.949 0.95  1.    0.727 0.909 0.98\n",
      " 0.92  0.94  0.687 0.859 1.    0.93  1.    0.667 0.909 0.97  0.93  0.8\n",
      " 0.384 0.899 0.99  0.81  1.    0.576 0.879 0.99  0.96  0.94  0.667 0.939\n",
      " 0.98  0.87  0.91  0.768 0.879 0.97  0.89  1.    0.818 0.869 0.96  0.9\n",
      " 0.95  0.545 0.96  1.   ]\n",
      "val_sens 0.799+/-0.153 : [0.9   0.909 0.417 0.8   0.875 0.9   1.    0.636 0.9   0.889 0.75  1.\n",
      " 0.6   0.7   1.    0.8   1.    0.583 0.727 0.889 0.778 0.818 0.538 0.727\n",
      " 1.    0.778 1.    0.429 0.818 1.    0.778 0.9   0.538 0.778 0.75  0.8\n",
      " 1.    0.636 0.778 0.889 0.818 0.833 0.583 0.75  1.    0.8   0.818 0.7\n",
      " 0.875 0.9   0.8   1.    0.545 0.778 0.889 0.9   0.909 0.7   0.889 0.875\n",
      " 0.818 0.8   0.545 0.778 1.    0.818 1.    0.583 0.818 0.889 0.9   0.727\n",
      " 0.429 0.818 1.    0.778 1.    0.375 0.75  0.9   0.818 0.9   0.636 0.889\n",
      " 0.889 0.818 0.818 0.583 0.7   0.9   0.818 1.    0.636 0.75  0.818 0.8\n",
      " 0.875 0.333 0.818 1.   ]\n",
      "val_spec 0.853+/-0.126 : [0.9   1.    0.5   0.9   0.833 0.9   0.833 0.778 1.    0.909 0.667 0.833\n",
      " 0.7   0.8   0.917 0.8   1.    0.75  0.889 0.909 0.727 0.889 0.714 0.889\n",
      " 1.    0.727 0.909 0.538 1.    1.    0.727 0.9   0.714 0.818 1.    0.8\n",
      " 1.    0.778 0.818 0.909 0.889 1.    0.75  1.    1.    0.8   0.889 0.8\n",
      " 0.833 1.    0.8   0.909 0.667 0.818 0.909 0.9   1.    0.8   0.909 0.833\n",
      " 0.889 0.8   0.667 0.818 1.    0.889 1.    0.75  1.    0.909 0.9   0.778\n",
      " 0.5   1.    0.917 0.727 1.    0.5   0.75  1.    0.889 0.9   0.778 0.909\n",
      " 0.909 0.889 0.889 0.75  0.8   1.    0.889 1.    0.778 1.    1.    0.8\n",
      " 0.75  0.455 1.    0.917]\n",
      "val_prec 0.843+/-0.140 : [0.9   1.    0.556 0.889 0.778 0.9   0.8   0.778 1.    0.889 0.6   0.8\n",
      " 0.667 0.778 0.889 0.8   1.    0.778 0.889 0.889 0.7   0.9   0.778 0.889\n",
      " 1.    0.7   0.9   0.333 1.    1.    0.7   0.9   0.778 0.778 1.    0.8\n",
      " 1.    0.778 0.778 0.889 0.9   1.    0.778 1.    1.    0.8   0.9   0.778\n",
      " 0.778 1.    0.8   0.9   0.667 0.778 0.889 0.9   1.    0.778 0.889 0.778\n",
      " 0.9   0.8   0.667 0.778 1.    0.9   1.    0.778 1.    0.889 0.9   0.8\n",
      " 0.667 1.    0.889 0.7   1.    0.333 0.667 1.    0.9   0.9   0.778 0.889\n",
      " 0.889 0.9   0.9   0.778 0.778 1.    0.9   1.    0.778 1.    1.    0.8\n",
      " 0.7   0.333 1.    0.889]\n",
      "Vanilla Results :\n",
      "best_epochs [6, 27, 2, 4, 31, 18, 29, 1, 4, 42, 21, 69, 4, 10, 36, 9, 38, 1, 21, 41, 5, 18, 3, 10, 26, 19, 60, 2, 14, 30, 11, 24, 1, 22, 28, 11, 37, 3, 7, 29, 19, 19, 4, 16, 34, 17, 51, 3, 4, 47, 2, 24, 2, 17, 42, 15, 28, 2, 5, 34, 12, 22, 2, 3, 60, 4, 19, 2, 8, 41, 7, 60, 1, 9, 29, 9, 21, 1, 9, 13, 15, 26, 4, 25, 26, 11, 33, 2, 20, 35, 16, 22, 1, 18, 30, 11, 20, 3, 3, 17]\n",
      "train_accuracy 0.813+/-0.168 : [0.65  0.988 0.512 0.7   0.95  0.825 0.988 0.54  0.637 0.988 0.925 1.\n",
      " 0.538 0.788 0.988 0.788 0.988 0.55  0.887 1.    0.712 0.913 0.625 0.75\n",
      " 0.962 0.925 1.    0.525 0.8   0.988 0.825 0.962 0.59  0.887 0.913 0.837\n",
      " 1.    0.613 0.688 0.962 0.9   0.938 0.55  0.95  1.    0.863 0.988 0.675\n",
      " 0.613 0.988 0.575 0.975 0.463 0.887 0.988 0.85  0.962 0.587 0.65  0.95\n",
      " 0.825 0.962 0.5   0.575 0.988 0.688 0.988 0.438 0.8   1.    0.7   1.\n",
      " 0.62  0.85  0.938 0.712 0.962 0.61  0.775 0.875 0.825 0.925 0.6   0.95\n",
      " 0.9   0.9   0.95  0.587 0.925 0.988 0.85  0.95  0.54  0.95  0.913 0.837\n",
      " 0.95  0.55  0.55  0.85 ]\n",
      "val_accuracy 0.621+/-0.121 : [0.5  0.65 0.45 0.35 0.8  0.7  0.6  0.75 0.7  0.75 0.7  0.75 0.45 0.7\n",
      " 0.75 0.5  0.6  0.45 0.65 0.8  0.7  0.65 0.5  0.6  0.6  0.65 0.7  0.45\n",
      " 0.65 0.7  0.7  0.55 0.5  0.65 0.75 0.65 0.7  0.65 0.45 0.65 0.65 0.6\n",
      " 0.55 0.5  0.75 0.65 0.75 0.45 0.45 0.85 0.5  0.65 0.6  0.65 0.85 0.65\n",
      " 0.65 0.55 0.45 0.65 0.65 0.7  0.35 0.5  0.8  0.7  0.7  0.65 0.5  0.65\n",
      " 0.6  0.7  0.3  0.6  0.65 0.45 0.7  0.4  0.75 0.65 0.65 0.65 0.45 0.8\n",
      " 0.6  0.75 0.7  0.6  0.6  0.85 0.7  0.65 0.45 0.6  0.75 0.65 0.55 0.5\n",
      " 0.35 0.8 ]\n",
      "val_auc 0.674+/-0.159 : [0.61  0.74  0.343 0.283 0.869 0.64  0.79  0.808 0.707 0.848 0.81  0.9\n",
      " 0.354 0.545 0.869 0.64  0.74  0.515 0.727 0.929 0.69  0.77  0.535 0.646\n",
      " 0.727 0.75  0.85  0.505 0.606 0.768 0.6   0.7   0.333 0.798 0.808 0.72\n",
      " 0.78  0.576 0.535 0.838 0.75  0.72  0.384 0.525 0.889 0.67  0.79  0.394\n",
      " 0.586 0.879 0.52  0.75  0.525 0.778 0.929 0.77  0.73  0.495 0.495 0.758\n",
      " 0.61  0.76  0.394 0.535 0.838 0.68  0.77  0.596 0.626 0.758 0.7   0.9\n",
      " 0.253 0.677 0.808 0.66  0.74  0.323 0.848 0.606 0.73  0.82  0.465 0.859\n",
      " 0.667 0.76  0.8   0.788 0.697 0.838 0.65  0.7   0.384 0.677 0.818 0.61\n",
      " 0.7   0.576 0.374 0.818]\n",
      "val_sens 0.597+/-0.174 : [0.5   0.667 0.417 0.    0.727 0.75  0.625 0.833 0.667 0.667 0.75  0.727\n",
      " 0.25  0.615 0.7   0.5   0.625 0.375 0.625 0.727 0.75  0.667 0.462 0.545\n",
      " 0.545 0.667 0.7   0.417 0.667 0.636 0.75  0.556 0.429 0.625 0.75  0.667\n",
      " 0.833 0.6   0.25  0.583 0.667 0.6   0.5   0.4   0.667 0.667 0.727 0.417\n",
      " 0.333 0.75  0.5   0.714 0.556 0.667 0.875 0.636 0.667 0.5   0.    0.583\n",
      " 0.667 0.75  0.333 0.4   0.692 0.75  0.75  0.583 0.444 0.583 0.667 0.75\n",
      " 0.143 0.667 0.6   0.444 0.75  0.333 0.833 0.583 0.714 0.636 0.429 0.857\n",
      " 0.556 0.778 0.667 1.    0.6   0.8   0.75  0.636 0.375 0.545 0.643 0.714\n",
      " 0.571 0.444 0.25  0.727]\n",
      "val_spec 0.649+/-0.131 : [0.5   0.636 0.5   0.438 0.889 0.667 0.583 0.714 0.727 0.875 0.667 0.778\n",
      " 0.5   0.857 0.8   0.5   0.583 0.5   0.667 0.889 0.667 0.636 0.571 0.667\n",
      " 0.667 0.636 0.7   0.5   0.643 0.778 0.667 0.545 0.538 0.667 0.75  0.636\n",
      " 0.643 0.7   0.5   0.75  0.636 0.6   0.583 0.533 0.875 0.636 0.778 0.5\n",
      " 0.5   1.    0.5   0.615 0.636 0.643 0.833 0.667 0.636 0.6   0.5   0.75\n",
      " 0.636 0.667 0.375 0.533 1.    0.667 0.667 0.75  0.545 0.75  0.571 0.667\n",
      " 0.385 0.588 0.7   0.455 0.667 0.455 0.714 0.75  0.615 0.667 0.5   0.769\n",
      " 0.636 0.727 0.75  0.579 0.6   0.9   0.667 0.667 0.5   0.667 1.    0.615\n",
      " 0.538 0.545 0.417 0.889]\n",
      "val_prec 0.568+/-0.216 : [0.4   0.6   0.556 0.    0.889 0.6   0.5   0.556 0.667 0.889 0.6   0.8\n",
      " 0.111 0.889 0.778 0.4   0.5   0.333 0.556 0.889 0.6   0.6   0.667 0.667\n",
      " 0.667 0.6   0.7   0.556 0.444 0.778 0.6   0.5   0.333 0.556 0.667 0.6\n",
      " 0.5   0.667 0.111 0.778 0.6   0.6   0.444 0.222 0.889 0.6   0.8   0.556\n",
      " 0.222 1.    0.4   0.5   0.556 0.444 0.778 0.7   0.6   0.556 0.    0.778\n",
      " 0.6   0.6   0.444 0.222 1.    0.6   0.6   0.778 0.444 0.778 0.4   0.6\n",
      " 0.111 0.222 0.667 0.4   0.6   0.333 0.556 0.778 0.5   0.7   0.667 0.667\n",
      " 0.556 0.7   0.8   0.111 0.333 0.889 0.6   0.7   0.333 0.667 1.    0.5\n",
      " 0.4   0.444 0.222 0.889]\n"
     ]
    }
   ],
   "source": [
    "print(\"GapNet Results :\")\n",
    "print(\"best_epochs {}\".format(gapnet_best_epochs))\n",
    "print(\"train_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_train_accuracies), np.std(gapnet_train_accuracies), np.round(gapnet_train_accuracies, 3)))\n",
    "print(\"val_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_accuracies), np.std(gapnet_val_accuracies), np.round(gapnet_val_accuracies, 3)))\n",
    "print(\"val_auc {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_aucs), np.std(gapnet_val_aucs), np.round(gapnet_val_aucs, 3)))\n",
    "print(\"val_sens {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_sensitivities), np.std(gapnet_val_sensitivities), np.round(gapnet_val_sensitivities, 3)))\n",
    "print(\"val_spec {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_specificities), np.std(gapnet_val_specificities), np.round(gapnet_val_specificities, 3)))\n",
    "print(\"val_prec {:.3f}+/-{:.3f} : {}\".format(np.mean(gapnet_val_precisions), np.std(gapnet_val_precisions), np.round(gapnet_val_precisions, 3)))\n",
    "print(\"Vanilla Results :\")\n",
    "print(\"best_epochs {}\".format(vanilla_best_epochs))\n",
    "print(\"train_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_train_accuracies), np.std(vanilla_train_accuracies), np.round(vanilla_train_accuracies, 3)))\n",
    "print(\"val_accuracy {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_accuracies), np.std(vanilla_val_accuracies), np.round(vanilla_val_accuracies, 3)))\n",
    "print(\"val_auc {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_aucs), np.std(vanilla_val_aucs), np.round(vanilla_val_aucs, 3)))\n",
    "print(\"val_sens {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_sensitivities), np.std(vanilla_val_sensitivities), np.round(vanilla_val_sensitivities, 3)))\n",
    "print(\"val_spec {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_specificities), np.std(vanilla_val_specificities), np.round(vanilla_val_specificities, 3)))\n",
    "print(\"val_prec {:.3f}+/-{:.3f} : {}\".format(np.mean(vanilla_val_precisions), np.std(vanilla_val_precisions), np.round(vanilla_val_precisions, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "from numpy import save\n",
    "import os\n",
    "#os.mkdir(\"simulation_result\")\n",
    "#save('simulation_result/gapnet_train_accuracies.npy', gapnet_train_accuracies)\n",
    "#save('simulation_result/gapnet_val_accuracies.npy', gapnet_val_accuracies)\n",
    "#save('simulation_result/gapnet_val_aucs.npy', gapnet_val_aucs)\n",
    "#save('simulation_result/gapnet_val_sensitivities.npy', gapnet_val_sensitivities)\n",
    "#save('simulation_result/gapnet_val_specificities.npy', gapnet_val_specificities)\n",
    "#save('simulation_result/gapnet_val_y_preds.npy', gapnet_val_y_preds)\n",
    "#save('simulation_result/gapnet_val_y_labels.npy', gapnet_val_y_labels)\n",
    "#save('simulation_result/vanilla_train_accuracies.npy', vanilla_train_accuracies)\n",
    "#save('simulation_result/vanilla_val_accuracies.npy', vanilla_val_accuracies)\n",
    "#save('simulation_result/vanilla_val_aucs.npy', vanilla_val_aucs)\n",
    "#save('simulation_result/vanilla_val_sensitivities.npy', vanilla_val_sensitivities)\n",
    "#save('simulation_result/vanilla_val_specificities.npy', vanilla_val_specificities)\n",
    "#save('simulation_result/vanilla_val_y_preds.npy', vanilla_val_y_preds)\n",
    "#save('simulation_result/vanilla_val_y_labels.npy', vanilla_val_y_labels)\n",
    "\n",
    "#save('simulation_result/X.npy', X)\n",
    "#save('simulation_result/y.npy', y)\n",
    "#save('simulation_result/X_overlap.npy', X_overlap)\n",
    "#save('simulation_result/y_overlap.npy', y_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous results\n",
    "#mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "#colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "#gapnet_val_y_labels = load('archived_data_and_results/simulation_result/gapnet_val_y_labels.npy')\n",
    "#gapnet_val_y_preds = load('archived_data_and_results/simulation_result/gapnet_val_y_preds.npy')\n",
    "#vanilla_val_y_labels = load('archived_data_and_results/simulation_result/vanilla_val_y_labels.npy')\n",
    "#vanilla_val_y_preds = load('archived_data_and_results/simulation_result/vanilla_val_y_preds.npy')\n",
    "#gapnet_val_aucs = load('archived_data_and_results/simulation_result/gapnet_val_aucs.npy')\n",
    "#vanilla_val_aucs = load('archived_data_and_results/simulation_result/vanilla_val_aucs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'auc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.6,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_avg(name, label, prediction, **kwargs):\n",
    "    plt.plot(np.linspace(0,1,11),np.linspace(0,1,11), linestyle='dashed',color='k', linewidth=1)\n",
    "    from sklearn import metrics\n",
    "    label= np.reshape(label,(runs,-1))\n",
    "    prediction= np.reshape(prediction,(runs,-1))\n",
    "    print(label.shape)\n",
    "    print(prediction.shape)\n",
    "    fp_all = []\n",
    "    tp_all = []\n",
    "    fp_base, tp_base, _ = metrics.roc_curve(label[0], prediction[0], drop_intermediate=False)\n",
    "    for i in range(num):\n",
    "        fp, tp, _ = metrics.roc_curve(label[i], prediction[i], drop_intermediate=False)\n",
    "        from scipy.interpolate import UnivariateSpline\n",
    "        old_indices = np.arange(0,len(fp))\n",
    "        new_length = len(fp_base)\n",
    "        new_indices = np.linspace(0,len(fp)-1,new_length)\n",
    "        spl = UnivariateSpline(old_indices,fp,k=5,s=0)\n",
    "        fp = spl(new_indices)\n",
    "        old_indices = np.arange(0,len(tp))\n",
    "        new_length = len(tp_base)\n",
    "        new_indices = np.linspace(0,len(tp)-1,new_length)\n",
    "        spl = UnivariateSpline(old_indices,tp,k=5,s=0)\n",
    "        tp = spl(new_indices)\n",
    "        fp_all.append(fp)\n",
    "        tp_all.append(tp)\n",
    "    plt.plot(np.mean(fp_all, axis=0), np.mean(tp_all, axis=0), label=name, linewidth=3, **kwargs)\n",
    "    plt.fill_between(np.mean(fp_all, axis=0), np.mean(tp_all, axis=0)-np.std(tp_all, axis=0), np.mean(tp_all, axis=0)+np.std(tp_all, axis=0), alpha=0.1, **kwargs)\n",
    "    plt.xlabel('FPR', fontsize=30)\n",
    "    plt.ylabel('TPR', fontsize=30)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "      ax.spines[axis].set_linewidth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gapnet_auc: 0.9215355279004415\n",
      "vallina_auc: 0.6112439123376623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('gapnet_auc:', roc_auc_score(gapnet_val_y_labels, gapnet_val_y_preds))\n",
    "print('vallina_auc:', roc_auc_score(vanilla_val_y_labels, vanilla_val_y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "(100, 20)\n",
      "(100, 20)\n",
      "(100, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_avg(\"0.89 GapNet\", gapnet_val_y_labels, gapnet_val_y_preds, linestyle='solid', color='darkorange')\n",
    "plot_roc_avg(\"0.62 Vanilla\", vanilla_val_y_labels, vanilla_val_y_preds, linestyle='solid', color='skyblue')\n",
    "leg = plt.legend(title=\"AUC\",loc='lower right',fontsize= '30')\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "leg.get_title().set_fontsize(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(gapnet_val_aucs, label='GapNet', bins=[0.2, 0.25, 0.3, 0.35,0.4, 0.45, 0.5,0.55, 0.6, 0.65, 0.7,0.75, 0.8,0.85, 0.9,0.95,1.0], alpha=0.5, color='darkorange')\n",
    "plt.hist(vanilla_val_aucs, label='Vanilla', bins=[0.2, 0.25, 0.3, 0.35,0.4, 0.45, 0.5,0.55, 0.6, 0.65, 0.7,0.75, 0.8,0.85, 0.9,0.95,1.0], alpha=0.5, color='skyblue')\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('auto')\n",
    "plt.xlabel('AUC', fontsize=30)\n",
    "plt.ylabel('Count', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylim([0,50])\n",
    "leg = plt.legend(fontsize= '40')\n",
    "leg.get_frame().set_linewidth(0.0)\n",
    "leg.get_title().set_fontsize(40)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(1)\n",
    "ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['sens', 'spec', 'acc', 'prec']\n",
    "gapnet_means = [0.802*100,0.872*100,0.833*100,0.865*100]\n",
    "vanilla_means = [0.593*100,0.653*100,0.623*100,0.580*100]\n",
    "gapnet_std = [0.132*100,0.101*100,0.113*100,0.111*100]\n",
    "vanilla_std = [0.169*100,0.139*100,0.130*100,0.212*100]\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "ax.bar(x - width/2, gapnet_means, width, label='GapNet', color='darkorange')\n",
    "ax.bar(x + width/2, vanilla_means, width, label='vanilla NN', color='skyblue')\n",
    "\n",
    "#ax.legend()\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(1)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylim([0,100])\n",
    "plt.ylabel('Percentage (%)', fontsize=30)\n",
    "#leg = plt.legend(fontsize= '30')\n",
    "#leg.get_frame().set_linewidth(0.0)\n",
    "#leg.get_title().set_fontsize(30)\n",
    "ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(gapnet_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(vanilla_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate Transactions Detected (True Negatives):  850\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  210\n",
      "Fraudulent Transactions Missed (False Negatives):  125\n",
      "Fraudulent Transactions Detected (True Positives):  815\n",
      "Total Fraudulent Transactions:  940\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(gapnet_val_y_labels, gapnet_val_y_preds, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate Transactions Detected (True Negatives):  551\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  329\n",
      "Fraudulent Transactions Missed (False Negatives):  542\n",
      "Fraudulent Transactions Detected (True Positives):  578\n",
      "Total Fraudulent Transactions:  1120\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm(vanilla_val_y_labels, vanilla_val_y_preds, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score: 22.68035956738896\n",
      "p-value: 7.001470541115719e-114\n"
     ]
    }
   ],
   "source": [
    "# perform delong test\n",
    "def auc(X, Y):\n",
    "    return 1/(len(X)*len(Y)) * sum([kernel(x, y) for x in X for y in Y])\n",
    "def kernel(X, Y):\n",
    "    return .5 if Y==X else int(Y < X)\n",
    "def structural_components(X, Y):\n",
    "    V10 = [1/len(Y) * sum([kernel(x, y) for y in Y]) for x in X]\n",
    "    V01 = [1/len(X) * sum([kernel(x, y) for x in X]) for y in Y]\n",
    "    return V10, V01\n",
    "    \n",
    "def get_S_entry(V_A, V_B, auc_A, auc_B):\n",
    "    return 1/(len(V_A)-1) * sum([(a-auc_A)*(b-auc_B) for a,b in zip(V_A, V_B)])\n",
    "def z_score(var_A, var_B, covar_AB, auc_A, auc_B):\n",
    "    return (auc_A - auc_B)/((var_A + var_B - 2*covar_AB)**(.5))\n",
    "\n",
    "#test https://biasedml.com/roc-comparison/\n",
    "import scipy.stats as st\n",
    "def group_preds_by_label(preds, actual):\n",
    "    X = [p for (p, a) in zip(preds, actual) if a]\n",
    "    Y = [p for (p, a) in zip(preds, actual) if not a]\n",
    "    return X, Y\n",
    "X_A, Y_A = group_preds_by_label(gapnet_val_y_preds, gapnet_val_y_labels)\n",
    "X_B, Y_B = group_preds_by_label(vanilla_val_y_preds, vanilla_val_y_labels)\n",
    "V_A10, V_A01 = structural_components(X_A, Y_A)\n",
    "V_B10, V_B01 = structural_components(X_B, Y_B)\n",
    "auc_A = auc(X_A, Y_A)\n",
    "auc_B = auc(X_B, Y_B)\n",
    "# Compute entries of covariance matrix S (covar_AB = covar_BA)\n",
    "var_A = (get_S_entry(V_A10, V_A10, auc_A, auc_A) * 1/len(V_A10)\n",
    "         + get_S_entry(V_A01, V_A01, auc_A, auc_A) * 1/len(V_A01))\n",
    "var_B = (get_S_entry(V_B10, V_B10, auc_B, auc_B) * 1/len(V_B10)\n",
    "         + get_S_entry(V_B01, V_B01, auc_B, auc_B) * 1/len(V_B01))\n",
    "covar_AB = (get_S_entry(V_A10, V_B10, auc_A, auc_B) * 1/len(V_A10)\n",
    "            + get_S_entry(V_A01, V_B01, auc_A, auc_B) * 1/len(V_A01))\n",
    "# Two tailed test\n",
    "z = z_score(var_A, var_B, covar_AB, auc_A, auc_B)\n",
    "p = st.norm.sf(abs(z))*2\n",
    "print(\"z-score:\", z)\n",
    "print(\"p-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
